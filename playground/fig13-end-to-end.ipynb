{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9badc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T00:15:43.617706Z",
     "iopub.status.busy": "2022-12-08T00:15:43.616466Z",
     "iopub.status.idle": "2022-12-08T00:15:48.654505Z",
     "shell.execute_reply": "2022-12-08T00:15:48.653531Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a17b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152abe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F, join, import_pickle\n",
    "from apperception.predicate import camera, objects, lit\n",
    "database.connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98619bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimized_ingestion.camera_config import camera_config\n",
    "from optimized_ingestion.payload import Payload\n",
    "from optimized_ingestion.pipeline import Pipeline\n",
    "from optimized_ingestion.stages.in_view import InView\n",
    "from optimized_ingestion.stages.decode_frame.parallel_decode_frame import ParallelDecodeFrame\n",
    "from optimized_ingestion.stages.decode_frame.decode_frame import DecodeFrame\n",
    "from optimized_ingestion.stages.detection_2d.yolo_detection import YoloDetection\n",
    "from optimized_ingestion.stages.detection_2d.object_type_filter import ObjectTypeFilter\n",
    "# from optimized_ingestion.stages.filter_car_facing_sideway import FilterCarFacingSideway\n",
    "from optimized_ingestion.stages.detection_estimation import DetectionEstimation\n",
    "from optimized_ingestion.stages.tracking_2d.strongsort import StrongSORT\n",
    "from optimized_ingestion.stages.detection_3d.from_2d_and_road import From2DAndRoad\n",
    "from optimized_ingestion.video import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa563843",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOSTON_VIDEOS = [\n",
    "#     \"scene-0757-CAM_FRONT\",\n",
    "    # \"scene-0103-CAM_FRONT\",\n",
    "#     \"scene-0553-CAM_FRONT\",\n",
    "    # \"scene-0665-CAM_FRONT\",\n",
    "    \"scene-0655-CAM_FRONT\",\n",
    "#     \"scene-0655-CAM_FRONT_RIGHT\",\n",
    "#     \"scene-0655-CAM_BACK_RIGHT\",\n",
    "#     \"scene-0553-CAM_FRONT_LEFT\"\n",
    "#     \"scene-0103-CAM_FRONT\"\n",
    "]\n",
    "\n",
    "NUSCENES_PROCESSED_DATA = \"NUSCENES_PROCESSED_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ScenicWorld'\n",
    "world = empty_world(name=name)\n",
    "\n",
    "obj1 = objects[0]\n",
    "obj2 = objects[1]\n",
    "cam = camera\n",
    "\n",
    "world = world.filter(\n",
    "    (obj1.id != obj2.id) &\n",
    "    F.like(obj1.type, 'car') &\n",
    "    F.like(obj2.type, 'car') &\n",
    "    F.angle_between(F.facing_relative(cam.ego, F.road_direction(cam.ego)), -15, 15) &\n",
    "    (F.distance(cam.ego, obj1.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj1.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    (F.distance(cam.ego, obj2.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj2.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    F.contains_all('intersection', [obj1.trans, obj2.trans]@cam.time) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, cam.ego), 50, 135) &\n",
    "    F.angle_between(F.facing_relative(obj2.trans@cam.time, cam.ego), -135, -50) &\n",
    "    (F.min_distance(cam.ego, 'intersection') < 10) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, obj2.trans@cam.time), 100, -100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67029d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.contains_all('intersection', [obj1.trans, obj2.trans]@cam.time) => lambda x: x.road_type == 'intersection'\n",
    "# F.min_distance(cam.ego, 'intersection') < 10 => InView(distance=10, segment_type='intersection')\n",
    "# F.like(obj1.type, 'vehicle%') => ObjectTypeFilter stage\n",
    "# F.distance(cam.ego, obj1.trans@cam.time) < 50 =>\n",
    "    # compute_distance(x.car_loc3d, x.ego_config.ego_translation) < 50\n",
    "from apperception.predicate import (Visitor, PredicateNode, CallNode,\n",
    "                                    CompOpNode, TableAttrNode, BinOpNode, LiteralNode)\n",
    "\n",
    "def in_view(pipeline, param):\n",
    "    pipeline.stages.insert(0, InView(**param))\n",
    "    \n",
    "def object_type(pipeline, param):\n",
    "    for i in range(len(pipeline.stages)):\n",
    "        if isinstance(pipeline.stages[i], YoloDetection):\n",
    "            pipeline.stages.insert(i+1, ObjectTypeFilter(param))\n",
    "    \n",
    "def road_type(pipeline, param):\n",
    "    for s in pipeline.stages:\n",
    "        if isinstance(s, DetectionEstimation):\n",
    "            s.filter(lambda x: x.road_type == param)\n",
    "            \n",
    "def distance_to_ego(pipeline, param):\n",
    "    for s in pipeline.stages:\n",
    "        if isinstance(s, DetectionEstimation):\n",
    "            s.filter(lambda x: compute_distance(\n",
    "                x.car_loc3d, x.ego_config.ego_translation) < param)\n",
    "            \n",
    "            \n",
    "ALL_MAPPING_RULES = {\n",
    "    'in_view': {'condition': lambda x: (isinstance(x, CompOpNode) and\n",
    "                                        isinstance(x.left, CallNode) and\n",
    "                                        isinstance(x.right, LiteralNode) and\n",
    "                                        x.left._fn[0].__name__ == 'fn' and\n",
    "                                        isinstance(x.left.params[0], TableAttrNode) and\n",
    "                                        x.left.params[0].name == 'egoTranslation' and\n",
    "                                        isinstance(x.left.params[1], LiteralNode)),\n",
    "                'param': lambda x: dict(segment_type=x.left.params[1].value, distance=x.right.value),\n",
    "                'pipeline': in_view},\n",
    "    'object_type': {'condition': lambda x: (isinstance(x, CallNode) and\n",
    "                                            x._fn[0].__name__ == 'like' and\n",
    "                                            x.params[0].name == 'objectType'),\n",
    "                    'param': lambda x: [x.params[1].value],\n",
    "                    'pipeline': object_type},\n",
    "    'road_type': {'condition': lambda x: (isinstance(x, CallNode) and\n",
    "                                         x._fn[0].__name__ == 'contains_all'),\n",
    "                  'param': lambda x: x.params[0].value,\n",
    "                  'pipeline': road_type},\n",
    "    'distance_to_ego': {'condition': lambda x: (isinstance(x, CompOpNode) and\n",
    "                                                isinstance(x.left, CallNode) and\n",
    "                                                isinstance(x.right, LiteralNode) and\n",
    "                                                x.left._fn[0].__name__ == 'fn' and\n",
    "                                                isinstance(x.left.params[0], TableAttrNode) and\n",
    "                                                x.left.params[0].name == 'egoTranslation' and\n",
    "                                                isinstance(x.left.params[1], BinOpNode)),\n",
    "                       'param': lambda x: x.right.value,\n",
    "                       'pipeline': distance_to_ego}\n",
    "}\n",
    "\n",
    "def pipeline_rule(pipeline, node):\n",
    "    for key, rule in ALL_MAPPING_RULES.items():\n",
    "        if rule['condition'](node):\n",
    "            param = rule['param'](node)\n",
    "            rule['pipeline'](pipeline, param)\n",
    "                \n",
    "class PipelineConstructor(Visitor[PredicateNode]):\n",
    "    \n",
    "    def add_pipeline(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        return self\n",
    "\n",
    "    def visit_CompOpNode(self, node: \"CompOpNode\"):\n",
    "        assert self.pipeline\n",
    "        pipeline_rule(self.pipeline, node)\n",
    "        self(node.left)\n",
    "        self(node.right)\n",
    "\n",
    "    def visit_CallNode(self, node: \"CallNode\"):\n",
    "        assert self.pipeline\n",
    "        pipeline_rule(self.pipeline, node)\n",
    "        for p in node.params:\n",
    "            self(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipeline(world):\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_filter(filter=ParallelDecodeFrame())\n",
    "    pipeline.add_filter(filter=YoloDetection())\n",
    "\n",
    "    pipeline.add_filter(filter=From2DAndRoad())\n",
    "    pipeline.add_filter(filter=DetectionEstimation())  # 5 Frame p Second\n",
    "    pipeline.add_filter(filter=StrongSORT())  # 2 Frame p Second\n",
    "    PipelineConstructor().add_pipeline(pipeline)(world.kwargs['predicate'])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_detection_info(tracking_result, detection_info_meta):\n",
    "    for detection_info in detection_info_meta[tracking_result.frame_idx]:\n",
    "        if detection_info.detection_id == tracking_result.detection_id:\n",
    "            return detection_info\n",
    "        \n",
    "def get_tracks(sortmeta, detection_estimation_meta):\n",
    "    trajectories = {}\n",
    "    for frame in sortmeta:\n",
    "        for obj_id, tracking_result in frame.items():\n",
    "            if obj_id not in trajectories:\n",
    "                trajectories[obj_id] = []\n",
    "            associated_detection_info = associate_detection_info(\n",
    "                tracking_result, detection_estimation_meta)\n",
    "            trajectories[obj_id].append((tracking_result, associated_detection_info))\n",
    "\n",
    "    for trajectory in trajectories.values():\n",
    "        last = len(trajectory) - 1\n",
    "        for i, t in enumerate(trajectory):\n",
    "            if i > 0:\n",
    "                t[0].prev = trajectory[i - 1][0]\n",
    "            if i < last:\n",
    "                t[0].next = trajectory[i + 1][0]\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def insert_trajectory(\n",
    "    database,\n",
    "    item_id: str,\n",
    "    camera_id: str,\n",
    "    object_type: str,\n",
    "    postgres_timestamps: List[str],\n",
    "    pairs: List[Tuple[float, float, float]],\n",
    "    itemHeading_list: List[int],\n",
    "    translation_list: List[Tuple[float, float, float]],\n",
    "    road_types: List[str],\n",
    "    roadpolygon_list: List[List[Tuple[float, float]]]\n",
    "):\n",
    "    ### Save camera config\n",
    "    PICKLE_DATA_PATH = '/data/apperception-data/processed/nuscenes/full-dataset-v1.0/Mini/videos/boston-seaport'\n",
    "    import_pickle(database, PICKLE_DATA_PATH)\n",
    "    traj_centroids = []\n",
    "    translations = []\n",
    "    itemHeadings = []\n",
    "    roadTypes = []\n",
    "    roadPolygons = []\n",
    "    prevTimestamp = None\n",
    "    for timestamp, current_point, curItemHeading, current_trans, cur_road_type, cur_roadpolygon in zip(\n",
    "        postgres_timestamps, pairs, itemHeading_list, translation_list, road_types, roadpolygon_list\n",
    "    ):\n",
    "        if prevTimestamp == timestamp:\n",
    "            continue\n",
    "        prevTimestamp = timestamp\n",
    "\n",
    "        # Construct trajectory\n",
    "        traj_centroids.append(f\"POINT Z ({join(current_point, ' ')})@{timestamp}\")\n",
    "        translations.append(f\"POINT Z ({join(current_trans, ' ')})@{timestamp}\")\n",
    "        itemHeadings.append(f\"{curItemHeading}@{timestamp}\")\n",
    "        roadTypes.append(f\"{cur_road_type}@{timestamp}\")\n",
    "#         polygon_point = ', '.join(join(cur_point, ' ') for cur_point in list(\n",
    "#             zip(*cur_roadpolygon.exterior.coords.xy)))\n",
    "#         roadPolygons.append(f\"Polygon (({polygon_point}))@{timestamp}\")\n",
    "\n",
    "    # Insert the item_trajectory separately\n",
    "    insert_trajectory = f\"\"\"\n",
    "    INSERT INTO Item_General_Trajectory (itemId, cameraId, objectType, roadTypes, trajCentroids,\n",
    "    translations, itemHeadings)\n",
    "    VALUES (\n",
    "        '{item_id}',\n",
    "        '{camera_id}',\n",
    "        '{object_type}',\n",
    "        ttext '{{[{', '.join(roadTypes)}]}}',\n",
    "        tgeompoint '{{[{', '.join(traj_centroids)}]}}',\n",
    "        tgeompoint '{{[{', '.join(translations)}]}}',\n",
    "        tfloat '{{[{', '.join(itemHeadings)}]}}'\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    database.execute(insert_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982adb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_trajectory(obj_id, track):\n",
    "    timestamps: List[str] = []\n",
    "    pairs: List[Tuple[float, float, float]] = []\n",
    "    itemHeadings: List[int] = []\n",
    "    translations: List[Tuple[float, float, float]] = []\n",
    "    road_types: List[str] = []\n",
    "    roadpolygons: List[List[Tuple[float, float]]] = []\n",
    "\n",
    "    for tracking_result_2d, detection_info in track:\n",
    "        if detection_info:\n",
    "            camera_id = detection_info.ego_config.camera_id\n",
    "            object_type = tracking_result_2d.object_type\n",
    "            timestamps.append(detection_info.timestamp)\n",
    "            pairs.append(detection_info.car_loc3d)\n",
    "            itemHeadings.append(detection_info.segment_heading)\n",
    "            translations.append(detection_info.ego_config.ego_translation)\n",
    "            road_types.append(detection_info.road_type)\n",
    "            roadpolygons.append(detection_info.road_polygon_info.polygon)\n",
    "    print([obj_id, camera_id, object_type, timestamps, pairs,\n",
    "            itemHeadings, translations, road_types, roadpolygons])\n",
    "    return [obj_id, camera_id, object_type, timestamps, pairs,\n",
    "            itemHeadings, translations, road_types, roadpolygons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline(frames, pipeline, insert=False):\n",
    "    output = pipeline.run(Payload(frames)).__dict__\n",
    "    metadata = output['metadata']\n",
    "    kept_fn = [i for i, val in enumerate(output['keep']) if val]\n",
    "\n",
    "    detection_estimation_meta = metadata['DetectionEstimation']\n",
    "    sortmeta = metadata['Tracking2D.StrongSORT']\n",
    "    tracks = get_tracks(sortmeta, detection_estimation_meta)\n",
    "    for obj_id, track in tracks.items():\n",
    "        if insert:\n",
    "            insert_trajectory(database, *format_trajectory(obj_id, track))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9724cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(world):\n",
    "    pipeline = construct_pipeline(world)\n",
    "    if NUSCENES_PROCESSED_DATA in os.environ:\n",
    "        DATA_DIR = os.environ[NUSCENES_PROCESSED_DATA]\n",
    "    else:\n",
    "        DATA_DIR = \"/work/apperception/data/nuScenes/full-dataset-v1.0/Mini\"\n",
    "    \n",
    "    video_dir = os.path.join(DATA_DIR, 'videos')\n",
    "    with open(os.path.join(video_dir, \"frames.pkl\"), \"rb\") as f:\n",
    "        videos = pickle.load(f)\n",
    "\n",
    "    num_video = 0\n",
    "    for name, video in videos.items():\n",
    "        if name not in BOSTON_VIDEOS:\n",
    "            continue\n",
    "    #     if not name.endswith('CAM_FRONT'):\n",
    "    #         continue\n",
    "    #     if 'CAM_FRONT' not in name:\n",
    "    #         continue\n",
    "\n",
    "        print(name, '--------------------------------------------------------------------------------')\n",
    "        frames = Video(\n",
    "            os.path.join(video_dir, video[\"filename\"]),\n",
    "            [camera_config(*f, 0) for f in video[\"frames\"]],\n",
    "            video[\"start\"],\n",
    "        )\n",
    "        process_pipeline(frames, pipeline)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c97b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_time_camId_filename = world.get_id_time_camId_filename(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_time_camId_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731c9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
