{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c9badc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T00:15:43.617706Z",
     "iopub.status.busy": "2022-12-08T00:15:43.616466Z",
     "iopub.status.idle": "2022-12-08T00:15:48.654505Z",
     "shell.execute_reply": "2022-12-08T00:15:48.653531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yongming/apperception\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152abe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2023-4-10 Python-3.10.6 torch-2.0.0+cu117 CUDA:0 (Tesla T4, 14966MiB)\n",
      "\n",
      "Using cache found in /home/yongming/apperception/weights/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"setuptools>=65.5.1\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /home/yongming/miniconda3/lib/python3.10/site-packages (65.6.3)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /home/yongming/apperception/weights/ultralytics_yolov5_master/requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5 üöÄ 2023-4-10 Python-3.10.6 torch-2.0.0+cu117 CUDA:0 (Tesla T4, 14966MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F\n",
    "from apperception.predicate import camera, objects\n",
    "from optimized_ingestion.utils.preprocess import preprocess\n",
    "database.connection\n",
    "from optimized_ingestion.cache import disable_cache\n",
    "disable_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922e90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUSCENES_PROCESSED_DATA = \"NUSCENES_PROCESSED_DATA\"\n",
    "if NUSCENES_PROCESSED_DATA in os.environ:\n",
    "    DATA_DIR = os.environ[NUSCENES_PROCESSED_DATA]\n",
    "else:\n",
    "    DATA_DIR = \"/work/apperception/data/nuScenes/full-dataset-v1.0/Mini\"\n",
    "NUSCENES_RAW_DATA = \"NUSCENES_RAW_DATA\"\n",
    "if NUSCENES_RAW_DATA in os.environ:\n",
    "    RAW_DATA_DIR = os.environ[NUSCENES_RAW_DATA]\n",
    "else:\n",
    "    RAW_DATA_DIR = \"/work/apperception/data/raw/nuScenes/full-dataset-v1.0/Mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574d9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from os.path import exists\n",
    "def construct_video(frames, size=(1600, 900), base=False, vid_name=None, vid_prefix=False):\n",
    "    unique_frames = []\n",
    "    for f in frames:\n",
    "        if f not in unique_frames:\n",
    "            unique_frames.append(f)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vid_name = \"./outputs/fig13_\"+vid_name if vid_name else f\"./outputs/{'fig13_base' if base else 'fig13_optimized'}.mp4\"\n",
    "    display_video = cv2.VideoWriter(vid_name,fourcc, 1, size)\n",
    "    for frame in unique_frames:\n",
    "        if vid_prefix:\n",
    "            img_path = os.path.join(RAW_DATA_DIR, 'sweeps/CAM_FRONT', frame)\n",
    "            if not exists(img_path):\n",
    "                img_path = os.path.join(RAW_DATA_DIR, 'samples/CAM_FRONT', frame)\n",
    "        else:\n",
    "            img_path = os.path.join(RAW_DATA_DIR, frame)\n",
    "        img = cv2.imread(img_path)\n",
    "        display_video.write(img)\n",
    "\n",
    "    display_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ScenicWorld'\n",
    "world = empty_world(name=name)\n",
    "\n",
    "obj1 = objects[0]\n",
    "obj2 = objects[1]\n",
    "cam = camera\n",
    "\n",
    "world = world.filter(\n",
    "    (obj1.id != obj2.id) &\n",
    "    (F.like(obj1.type, 'car') | F.like(obj1.type, 'truck')) &\n",
    "    (F.like(obj2.type, 'car') | F.like(obj2.type, 'truck')) &\n",
    "    F.angle_between(F.facing_relative(cam.ego, F.road_direction(cam.ego)), -15, 15) &\n",
    "    (F.distance(cam.ego, obj1.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj1.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    (F.distance(cam.ego, obj2.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj2.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    F.contains_all('intersection', [obj1.trans, obj2.trans]@cam.time) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, cam.ego), 40, 135) &\n",
    "    F.angle_between(F.facing_relative(obj2.trans@cam.time, cam.ego), -135, -50) &\n",
    "    (F.min_distance(cam.ego, 'intersection') < 10) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, obj2.trans@cam.time), 100, -100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20af03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2023-4-10 Python-3.10.6 torch-2.0.0+cu117 CUDA:0 (Tesla T4, 14966MiB)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"setuptools>=65.5.1\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /home/yongming/miniconda3/lib/python3.10/site-packages (65.6.3)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /home/yongming/apperception/weights/ultralytics_yolov5_master/requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5 üöÄ 2023-4-10 Python-3.10.6 torch-2.0.0+cu117 CUDA:0 (Tesla T4, 14966MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene-0655-CAM_FRONT --------------------------------------------------------------------------------\n",
      "  filtered frames: 66.24472573839662%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filtered frames: 66.24472573839662%\n",
      "  filtered frames: 66.24472573839662%\n",
      "  filtered frames: 66.24472573839662%\n",
      "  filtered frames: 66.24472573839662%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 978, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/yongming/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_40355/3677074789.py\", line 1, in <module>\n",
      "    preprocess(world, DATA_DIR, ['scene-0655-CAM_FRONT'], base=False)\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/utils/preprocess.py\", line 38, in preprocess\n",
      "    process_pipeline(name, frames, pipeline, base)\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/utils/process_pipeline.py\", line 177, in process_pipeline\n",
      "    output = pipeline.run(Payload(frames)).__dict__\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/pipeline.py\", line 22, in run\n",
      "    payload = payload.filter(stage)\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/payload.py\", line 35, in filter\n",
      "    keep, metadata = filter.run(self)\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/stages/stage.py\", line 28, in run\n",
      "    out = self._run(payload)\n",
      "  File \"/home/yongming/apperception/optimized_ingestion/stages/detection_estimation/__init__.py\", line 45, in _run\n",
      "    logger.info(\"ego_speed: \", ego_speed)\n",
      "Message: 'ego_speed: '\n",
      "Arguments: (8.223887668036209,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_detections [[0, 530, 480, 692, 584, 'intersection', (array('d', [363.0340999493258, 1337.7113240288093, 1692.9239863364926, 1790.0120384724512, 2032.4876728386516, 363.0340999493258]), array('d', [863.9872080248853, 869.7245561458105, 896.1907881034633, 933.9768149085215, 1035.6408551785412, 863.9872080248853]))]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sorted_ego_config_length 237\n",
      "investigation_frame_nums [[1, None, None], [2, None, None], [10, 'ego_exit_segment', None], [11, None, None], [12, None, None], [13, None, None], [14, None, None], [15, None, None], [16, None, None], [17, None, None], [20, 'meet_up', ((1, 359), (269, 639))], [21, 'car_exit_segment', ((458, 465), (551, 537))], [22, None, None], [23, None, None], [24, None, None], [25, None, None], [26, None, None], [27, None, None], [34, 'car_exit_segment', ((558, 449), (632, 521))], [35, 'ego_exit_segment', None], [37, 'car_exit_segment', ((1204, 459), (1445, 573))], [46, 'car_exit_segment', ((1236, 454), (1498, 579))], [55, 'car_exit_segment', ((1047, 474), (1218, 534))], [56, 'car_exit_segment', ((1105, 469), (1277, 547))], [57, 'car_exit_segment', ((496, 472), (580, 521))], [58, 'car_exit_segment', ((134, 431), (453, 608))], [65, 'ego_exit_segment', None], [113, None, None], [114, None, None], [115, None, None], [116, None, None], [117, None, None], [118, None, None], [119, None, None], [120, None, None], [121, None, None], [122, None, None], [132, 'ego_exit_segment', None], [133, 'car_exit_segment', ((1235, 467), (1308, 506))], [137, 'car_exit_segment', ((689, 459), (765, 521))], [138, 'car_exit_segment', ((1204, 483), (1266, 521))], [140, 'car_exit_segment', ((684, 470), (764, 531))], [141, None, None], [142, None, None], [143, None, None], [153, 'ego_exit_segment', None], [167, None, None], [168, None, None], [169, None, None], [170, None, None], [171, None, None], [172, None, None], [173, None, None], [174, None, None], [175, None, None], [182, 'ego_exit_segment', None], [183, None, None], [184, None, None], [185, None, None], [186, None, None], [187, None, None], [188, None, None], [189, None, None], [195, 'ego_exit_segment', None], [209, 'car_exit_segment', ((728, 463), (785, 509))], [211, 'car_exit_segment', ((724, 462), (782, 507))], [213, 'car_exit_segment', ((713, 453), (778, 507))], [214, None, None], [215, None, None], [216, None, None], [217, 'car_exit_segment', ((79, 485), (203, 543))], [220, 'car_exit_segment', ((52, 487), (178, 539))], [223, 'ego_exit_segment', None], [236, 'exit_view', ((624, 453), (746, 550))]]\n",
      "number of skipped 163\n",
      "{None: 47, 'ego_exit_segment': 8, 'meet_up': 1, 'car_exit_segment': 17, 'exit_view': 1}\n",
      "total_run_time 5.842343330383301\n",
      "total_detection_time 4.695188760757446\n",
      "total_generate_sample_plan_time 1.1397371292114258\n",
      "YOLOv5 üöÄ 2023-4-10 Python-3.10.6 torch-2.0.0+cu117 CUDA:0 (Tesla T4, 14966MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filtered frames: 31.223628691983123%\n",
      "Successfully loaded pretrained weights from \"/home/yongming/apperception/weights/osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "  filtered frames: 31.223628691983123%\n",
      "  filtered frames: 31.223628691983123%\n",
      "  filtered frames: 31.223628691983123%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Tracking3D.From2DAndRoad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscene-0655-CAM_FRONT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apperception/optimized_ingestion/utils/preprocess.py:38\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(world, data_dir, video_names, base, benchmark_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m     frames \u001b[38;5;241m=\u001b[39m Video(\n\u001b[1;32m     33\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m, video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     34\u001b[0m         [camera_config(name, \u001b[38;5;241m*\u001b[39mf[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     35\u001b[0m         video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     36\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     num_video \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_video: \u001b[39m\u001b[38;5;124m\"\u001b[39m, num_video)\n",
      "File \u001b[0;32m~/apperception/optimized_ingestion/utils/process_pipeline.py:180\u001b[0m, in \u001b[0;36mprocess_pipeline\u001b[0;34m(video_name, frames, pipeline, base)\u001b[0m\n\u001b[1;32m    178\u001b[0m metadata \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    179\u001b[0m ego_meta \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39minterpolated_frames\n\u001b[0;32m--> 180\u001b[0m sortmeta \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTracking3D.From2DAndRoad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    181\u001b[0m segment_trajectory_mapping \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentTrajectory.FromTracking3D\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    182\u001b[0m tracks \u001b[38;5;241m=\u001b[39m get_tracks(sortmeta, ego_meta, segment_trajectory_mapping, base)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Tracking3D.From2DAndRoad'"
     ]
    }
   ],
   "source": [
    "preprocess(world, DATA_DIR, ['scene-0655-CAM_FRONT'], base=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e90463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "investigation =[[0, 530, 480, 692, 584, 'intersection'], [1, 0, 452, 210, 647, 'intersection'], [2, 1329, 475, 1599, 612, 'intersection'], [3, 196, 430, 467, 598, 'intersection'], [4, 1105, 469, 1277, 547, 'intersection'], [5, 458, 476, 531, 550, 'intersection'], [6, 724, 480, 763, 510, 'intersection'], [7, 511, 470, 591, 532, 'intersection'], [8, 1292, 475, 1487, 579, 'intersection'], [9, 773, 473, 811, 498, 'intersection'], [10, 1055, 478, 1095, 507, 'intersection'], [11, 849, 476, 875, 500, 'intersection']]\n",
    "target = [[0, 530, 480, 692, 584, 'intersection', \n",
    "           np.array([list(zip([363.0340999493258, 1337.7113240288093, 1692.9239863364926,\n",
    "                               1790.0120384724512, 2032.4876728386516, 363.0340999493258],\n",
    "                              [863.9872080248853, 869.7245561458105, 896.1907881034633,\n",
    "                               933.9768149085215, 1035.6408551785412, 863.9872080248853]))]).reshape(-1,1,2)]]\n",
    "\n",
    "skipping_info_0655=[[1, None, None], [2, None, None], [10, 'ego_exit_segment', None], [11, None, None], [12, None, None], [13, None, None], [14, None, None], [15, None, None], [16, None, None], [17, None, None], [20, 'meet_up', ((-133, 219), (135, 499))], [21, 'car_exit_segment', ((412, 429), (504, 501))], [22, None, None], [23, None, None], [24, None, None], [25, None, None], [26, None, None], [27, None, None], [34, 'car_exit_segment', ((521, 413), (595, 485))], [35, 'ego_exit_segment', None], [37, 'car_exit_segment', ((1084, 402), (1324, 516))], [46, 'car_exit_segment', ((1105, 392), (1367, 516))], [55, 'car_exit_segment', ((962, 444), (1132, 504))], [56, 'car_exit_segment', ((1019, 430), (1191, 508))], [57, 'car_exit_segment', ((454, 448), (538, 496))], [58, 'car_exit_segment', ((-25, 343), (293, 519))], [65, 'ego_exit_segment', None], [113, None, None], [114, None, None], [115, None, None], [116, None, None], [117, None, None], [118, None, None], [119, None, None], [120, None, None], [121, None, None], [122, None, None], [132, 'ego_exit_segment', None], [133, 'car_exit_segment', ((1199, 448), (1271, 486))], [137, 'car_exit_segment', ((651, 428), (727, 490))], [138, 'car_exit_segment', ((1173, 464), (1235, 502))], [140, 'car_exit_segment', ((644, 440), (724, 500))], [141, None, None], [142, None, None], [143, None, None], [153, 'ego_exit_segment', None], [167, None, None], [168, None, None], [169, None, None], [170, None, None], [171, None, None], [172, None, None], [173, None, None], [174, None, None], [175, None, None], [182, 'ego_exit_segment', None], [183, None, None], [184, None, None], [185, None, None], [186, None, None], [187, None, None], [188, None, None], [189, None, None], [195, 'ego_exit_segment', None], [209, 'car_exit_segment', ((700, 440), (756, 486))], [211, 'car_exit_segment', ((695, 440), (753, 484))], [213, 'car_exit_segment', ((681, 426), (745, 480))], [214, None, None], [215, None, None], [216, None, None], [217, 'car_exit_segment', ((17, 456), (141, 514))], [220, 'car_exit_segment', ((-11, 461), (115, 513))], [223, 'ego_exit_segment', None], [236, 'exit_view', ((563, 405), (685, 501))]]\n",
    "\n",
    "def investigation_skipping(video_filename, skipping_info):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vid_name = f\"./outputs/{video_filename}_skipping_investigation.mp4\"\n",
    "    display_video = cv2.VideoWriter(vid_name, fourcc, 1, (1600, 900))\n",
    "    video_path = os.path.join(DATA_DIR, \"videos\", video_filename)\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    current_frame_num, current_action, current_target_bbox = skipping_info[0]\n",
    "    current_skipping_info_idx = 0\n",
    "    while(vidcap.isOpened()):\n",
    "        success, image = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        next_skipping_info_idx = current_skipping_info_idx + 1\n",
    "        if next_skipping_info_idx < len(skipping_info):\n",
    "            next_frame_num, next_action, next_target_bbox = skipping_info[next_skipping_info_idx]\n",
    "        if frame_idx < next_frame_num:\n",
    "            if current_action:\n",
    "                image = cv2.putText(image, current_action, (50, 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            if current_target_bbox:\n",
    "                x, y = current_target_bbox[0]\n",
    "                x_w, y_h = current_target_bbox[1]\n",
    "                cv2.rectangle(image, (x+(x_w-x)//2, y+(y_h-y)//2), (x_w+(x_w-x)//2, y_h+(y_h-y)//2), (0,255,0), 4)\n",
    "        if frame_idx == 55:\n",
    "            for e in target:\n",
    "                did, x, y, x_w, y_h, t, pts1 = e\n",
    "                cv2.putText(image, f'obj_{did}', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, t, (x, y_h), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.rectangle(image, (x,y), (x_w, y_h), (0,255,0), 2)\n",
    "                cv2.polylines(image, np.int32([pts1]), isClosed=True, color=(255,0,0), thickness = 2)\n",
    "            cv2.imwrite('./outputs/0655_frame_55_skipping_investigation.jpg', image)\n",
    "        display_video.write(image)\n",
    "        frame_idx += 1\n",
    "        if frame_idx == next_frame_num:\n",
    "            current_skipping_info_idx = next_skipping_info_idx\n",
    "            current_action = next_action\n",
    "            current_target_bbox = next_target_bbox\n",
    "    display_video.release()\n",
    "investigation_skipping('boston-seaport-scene-0655-CAM_FRONT.mp4', skipping_info_0655)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d62d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute: filter\n",
      "execute: get_id_time_camId_filename\n",
      "get_id_time_camId_filename \n",
      "        SELECT t0.itemId, cameras.timestamp, cameras.cameraId, cameras.filename\n",
      "        FROM (SELECT * FROM item_general_trajectory) as t0\n",
      "        \n",
      "        JOIN Cameras USING (cameraId)\n",
      "        WHERE ((t0.objectType LIKE 'car' OR t0.objectType LIKE 'truck') AND angleBetween(facingRelative(egoHeading,roadDirection(egoTranslation,egoHeading)),-15,15) AND (ST_Distance(egoTranslation,valueAtTimestamp(t0.translations,timestamp))<50) AND (EXISTS(SELECT 1\n",
      "            FROM SegmentPolygon\n",
      "            WHERE\n",
      "                SegmentPolygon.__RoadType__intersection__ AND\n",
      "                ST_Covers(SegmentPolygon.elementPolygon, valueAtTimestamp(t0.translations,timestamp))\n",
      "        )) AND angleBetween(facingRelative((headingAtTimestamp(t0.itemHeadings,timestamp))::real,egoHeading),135,225) AND (minDistance(egoTranslation,'intersection')<10))\n",
      "        \n",
      "NOTICE:  table \"cameras\" does not exist, skipping\n",
      "\n",
      "NOTICE:  table \"item_general_trajectory\" does not exist, skipping\n",
      "\n",
      "done execute node\n",
      "Result length: 0\n",
      "optimized query time: 351.32667875289917\n"
     ]
    }
   ],
   "source": [
    "optimized_query_start = time.time()\n",
    "id_time_camId_filename = world.get_id_time_camId_filename(2)\n",
    "print(f'optimized query time: {time.time()-optimized_query_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_results = [e[4] for e in id_time_camId_filename]\n",
    "construct_video(optimized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be10787",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(world, DATA_DIR, ['scene-0757-CAM_FRONT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query_start = time.time()\n",
    "id_time_camId_filename_base = world.get_id_time_camId_filename(2)\n",
    "print(f'based query time: {time.time()-base_query_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_time_camId_filename_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = [e[4] for e in id_time_camId_filename_base]\n",
    "construct_video(base_results, base=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative = [f for f in base_results if f not in optimized_results]\n",
    "print(len(false_negative))\n",
    "construct_video(false_negative, vid_name=\"false_negative.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19be7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive = [f for f in optimized_results if f not in base_results]\n",
    "print(len(false_positive))\n",
    "construct_video(false_positive, vid_name=\"false_positive.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d9bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f6ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
