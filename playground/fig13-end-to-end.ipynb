{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> optimized_ingestion
   "id": "c5c9badc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T00:15:43.617706Z",
     "iopub.status.busy": "2022-12-08T00:15:43.616466Z",
     "iopub.status.idle": "2022-12-08T00:15:48.654505Z",
     "shell.execute_reply": "2022-12-08T00:15:48.653531Z"
    }
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/apperception/apperception\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> optimized_ingestion
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> optimized_ingestion
   "id": "21a17b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
<<<<<<< HEAD
    "import pickle"
=======
    "import pickle\n"
>>>>>>> optimized_ingestion
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "152abe6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<connection object at 0x7fb3ff40ee80; dsn: 'user=docker password=xxx dbname=mobilitydb host=localhost port=25441', closed: 0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F, import_pickle\n",
    "from apperception.predicate import camera, objects\n",
=======
   "execution_count": null,
   "id": "152abe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F, join, import_pickle\n",
    "from apperception.predicate import camera, objects, lit\n",
>>>>>>> optimized_ingestion
    "database.connection"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": null,
   "id": "98619bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimized_ingestion.camera_config import camera_config\n",
    "from optimized_ingestion.payload import Payload\n",
    "from optimized_ingestion.pipeline import Pipeline\n",
    "from optimized_ingestion.stages.in_view import InView\n",
    "from optimized_ingestion.stages.decode_frame.parallel_decode_frame import ParallelDecodeFrame\n",
    "from optimized_ingestion.stages.decode_frame.decode_frame import DecodeFrame\n",
    "from optimized_ingestion.stages.detection_2d.yolo_detection import YoloDetection\n",
    "from optimized_ingestion.stages.detection_2d.object_type_filter import ObjectTypeFilter\n",
    "# from optimized_ingestion.stages.filter_car_facing_sideway import FilterCarFacingSideway\n",
    "from optimized_ingestion.stages.detection_estimation import DetectionEstimation\n",
    "from optimized_ingestion.stages.tracking_2d.strongsort import StrongSORT\n",
    "from optimized_ingestion.stages.detection_3d.from_2d_and_road import From2DAndRoad\n",
    "from optimized_ingestion.video import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa563843",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOSTON_VIDEOS = [\n",
    "#     \"scene-0757-CAM_FRONT\",\n",
    "    # \"scene-0103-CAM_FRONT\",\n",
    "#     \"scene-0553-CAM_FRONT\",\n",
    "    # \"scene-0665-CAM_FRONT\",\n",
    "    \"scene-0655-CAM_FRONT\",\n",
    "#     \"scene-0655-CAM_FRONT_RIGHT\",\n",
    "#     \"scene-0655-CAM_BACK_RIGHT\",\n",
    "#     \"scene-0553-CAM_FRONT_LEFT\"\n",
    "#     \"scene-0103-CAM_FRONT\"\n",
    "]\n",
    "\n",
    "NUSCENES_PROCESSED_DATA = \"NUSCENES_PROCESSED_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> optimized_ingestion
   "id": "533a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ScenicWorld'\n",
    "world = empty_world(name=name)\n",
    "\n",
    "obj1 = objects[0]\n",
    "obj2 = objects[1]\n",
    "cam = camera\n",
    "\n",
    "world = world.filter(\n",
    "    (obj1.id != obj2.id) &\n",
    "    F.like(obj1.type, 'car') &\n",
    "    F.like(obj2.type, 'car') &\n",
    "    F.angle_between(F.facing_relative(cam.ego, cam.roadDirection), -15, 15) &\n",
    "    (F.distance(cam.ego, obj1.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj1.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    (F.distance(cam.ego, obj2.trans@cam.time) < 50) &\n",
    "    (F.view_angle(obj2.trans@cam.time, cam.ego) < 70 / 2.0) &\n",
    "    F.contains_all('intersection', [obj1.trans, obj2.trans]@cam.time) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, cam.ego), 50, 135) &\n",
    "    F.angle_between(F.facing_relative(obj2.trans@cam.time, cam.ego), -135, -50) &\n",
    "    (F.min_distance(cam.ego, 'intersection') < 10) &\n",
    "    F.angle_between(F.facing_relative(obj1.trans@cam.time, obj2.trans@cam.time), 100, -100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "9be10787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/apperception/.installs/mambaforge/envs/apperception/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5 ðŸš€ 2022-11-23 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12196MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/apperception/apperception/weights/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-11-30 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12196MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2022-11-23 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12196MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n",
      "New camera inserted successfully.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/apperception/weights/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-11-30 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12196MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene-0655-CAM_FRONT --------------------------------------------------------------------------------\n",
      "Stage:  DecodeFrame.ParallelDecodeFrame\n",
      "None\n",
      "396\n",
      "  filtered frames: 100.0%\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "Stage:  Detection2D.YoloDetection\n",
      "None\n",
      "396\n",
      "  filtered frames: 100.0%\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "Stage:  Detection3D.From2DAndRoad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "396it [00:00, 1901.96it/s]\n",
      "YOLOv5 ðŸš€ 2022-11-23 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA TITAN Xp, 12196MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "396\n",
      "  filtered frames: 100.0%\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "Stage:  Tracking2D.StrongSORT\n",
      "Successfully loaded pretrained weights from \"/data/apperception/apperception/weights/osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [05:25<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "396\n",
      "  filtered frames: 100.0%\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "Stage:  Tracking3D.From2DAndRoad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [00:00<00:00, 6408.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "396\n",
      "  filtered frames: 100.0%\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
      "KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DetectionEstimation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimized_ingestion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/apperception/apperception/optimized_ingestion/utils/preprocess.py:55\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(world, base)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m frames \u001b[38;5;241m=\u001b[39m Video(\n\u001b[1;32m     51\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos/boston-seaport\u001b[39m\u001b[38;5;124m\"\u001b[39m, video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     52\u001b[0m     [camera_config(\u001b[38;5;241m*\u001b[39mf, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     53\u001b[0m     video[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 55\u001b[0m \u001b[43mprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/apperception/apperception/optimized_ingestion/utils/process_pipeline.py:139\u001b[0m, in \u001b[0;36mprocess_pipeline\u001b[0;34m(video_name, frames, pipeline, base)\u001b[0m\n\u001b[1;32m    136\u001b[0m output \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mrun(Payload(frames))\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m    137\u001b[0m metadata \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 139\u001b[0m detection_estimation_meta \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDetectionEstimation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    140\u001b[0m sortmeta \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTracking2D.StrongSORT\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    141\u001b[0m tracks \u001b[38;5;241m=\u001b[39m get_tracks(frames\u001b[38;5;241m.\u001b[39mcamera_config, sortmeta, detection_estimation_meta, base)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DetectionEstimation'"
     ]
    }
   ],
   "source": [
    "from optimized_ingestion.utils.preprocess import preprocess\n",
    "preprocess(world)"
=======
   "execution_count": null,
   "id": "b67029d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.contains_all('intersection', [obj1.trans, obj2.trans]@cam.time) => lambda x: x.road_type == 'intersection'\n",
    "# F.min_distance(cam.ego, 'intersection') < 10 => InView(distance=10, segment_type='intersection')\n",
    "# F.like(obj1.type, 'vehicle%') => ObjectTypeFilter stage\n",
    "# F.distance(cam.ego, obj1.trans@cam.time) < 50 =>\n",
    "    # compute_distance(x.car_loc3d, x.ego_config.ego_translation) < 50\n",
    "from apperception.predicate import (Visitor, PredicateNode, CallNode,\n",
    "                                    CompOpNode, TableAttrNode, BinOpNode, LiteralNode)\n",
    "\n",
    "def in_view(pipeline, param):\n",
    "    pipeline.stages.insert(0, InView(**param))\n",
    "    \n",
    "def object_type(pipeline, param):\n",
    "    for i in range(len(pipeline.stages)):\n",
    "        if isinstance(pipeline.stages[i], YoloDetection):\n",
    "            pipeline.stages.insert(i+1, ObjectTypeFilter(param))\n",
    "    \n",
    "def road_type(pipeline, param):\n",
    "    for s in pipeline.stages:\n",
    "        if isinstance(s, DetectionEstimation):\n",
    "            s.filter(lambda x: x.road_type == param)\n",
    "            \n",
    "def distance_to_ego(pipeline, param):\n",
    "    for s in pipeline.stages:\n",
    "        if isinstance(s, DetectionEstimation):\n",
    "            s.filter(lambda x: compute_distance(\n",
    "                x.car_loc3d, x.ego_config.ego_translation) < param)\n",
    "            \n",
    "            \n",
    "ALL_MAPPING_RULES = {\n",
    "    'in_view': {'condition': lambda x: (isinstance(x, CompOpNode) and\n",
    "                                        isinstance(x.left, CallNode) and\n",
    "                                        isinstance(x.right, LiteralNode) and\n",
    "                                        x.left._fn[0].__name__ == 'fn' and\n",
    "                                        isinstance(x.left.params[0], TableAttrNode) and\n",
    "                                        x.left.params[0].name == 'egoTranslation' and\n",
    "                                        isinstance(x.left.params[1], LiteralNode)),\n",
    "                'param': lambda x: dict(segment_type=x.left.params[1].value, distance=x.right.value),\n",
    "                'pipeline': in_view},\n",
    "    'object_type': {'condition': lambda x: (isinstance(x, CallNode) and\n",
    "                                            x._fn[0].__name__ == 'like' and\n",
    "                                            x.params[0].name == 'objectType'),\n",
    "                    'param': lambda x: [x.params[1].value],\n",
    "                    'pipeline': object_type},\n",
    "    'road_type': {'condition': lambda x: (isinstance(x, CallNode) and\n",
    "                                         x._fn[0].__name__ == 'contains_all'),\n",
    "                  'param': lambda x: x.params[0].value,\n",
    "                  'pipeline': road_type},\n",
    "    'distance_to_ego': {'condition': lambda x: (isinstance(x, CompOpNode) and\n",
    "                                                isinstance(x.left, CallNode) and\n",
    "                                                isinstance(x.right, LiteralNode) and\n",
    "                                                x.left._fn[0].__name__ == 'fn' and\n",
    "                                                isinstance(x.left.params[0], TableAttrNode) and\n",
    "                                                x.left.params[0].name == 'egoTranslation' and\n",
    "                                                isinstance(x.left.params[1], BinOpNode)),\n",
    "                       'param': lambda x: x.right.value,\n",
    "                       'pipeline': distance_to_ego}\n",
    "}\n",
    "\n",
    "def pipeline_rule(pipeline, node):\n",
    "    for key, rule in ALL_MAPPING_RULES.items():\n",
    "        if rule['condition'](node):\n",
    "            param = rule['param'](node)\n",
    "            rule['pipeline'](pipeline, param)\n",
    "                \n",
    "class PipelineConstructor(Visitor[PredicateNode]):\n",
    "    \n",
    "    def add_pipeline(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        return self\n",
    "\n",
    "    def visit_CompOpNode(self, node: \"CompOpNode\"):\n",
    "        assert self.pipeline\n",
    "        pipeline_rule(self.pipeline, node)\n",
    "        self(node.left)\n",
    "        self(node.right)\n",
    "\n",
    "    def visit_CallNode(self, node: \"CallNode\"):\n",
    "        assert self.pipeline\n",
    "        pipeline_rule(self.pipeline, node)\n",
    "        for p in node.params:\n",
    "            self(p)\n"
>>>>>>> optimized_ingestion
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "8ff5d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_time_camId_filename"
=======
   "id": "3f19eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipeline(world):\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_filter(filter=ParallelDecodeFrame())\n",
    "    pipeline.add_filter(filter=YoloDetection())\n",
    "\n",
    "    pipeline.add_filter(filter=From2DAndRoad())\n",
    "    pipeline.add_filter(filter=DetectionEstimation())  # 5 Frame p Second\n",
    "    pipeline.add_filter(filter=StrongSORT())  # 2 Frame p Second\n",
    "    PipelineConstructor().add_pipeline(pipeline)(world.kwargs['predicate'])\n",
    "    return pipeline"
>>>>>>> optimized_ingestion
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "469cee14",
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "id": "b491a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_detection_info(tracking_result, detection_info_meta):\n",
    "    for detection_info in detection_info_meta[tracking_result.frame_idx]:\n",
    "        if detection_info.detection_id == tracking_result.detection_id:\n",
    "            return detection_info\n",
    "        \n",
    "def get_tracks(sortmeta, detection_estimation_meta):\n",
    "    trajectories = {}\n",
    "    for frame in sortmeta:\n",
    "        for obj_id, tracking_result in frame.items():\n",
    "            if obj_id not in trajectories:\n",
    "                trajectories[obj_id] = []\n",
    "            associated_detection_info = associate_detection_info(\n",
    "                tracking_result, detection_estimation_meta)\n",
    "            trajectories[obj_id].append((tracking_result, associated_detection_info))\n",
    "\n",
    "    for trajectory in trajectories.values():\n",
    "        last = len(trajectory) - 1\n",
    "        for i, t in enumerate(trajectory):\n",
    "            if i > 0:\n",
    "                t[0].prev = trajectory[i - 1][0]\n",
    "            if i < last:\n",
    "                t[0].next = trajectory[i + 1][0]\n",
    "    return trajectories"
   ]
>>>>>>> optimized_ingestion
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "cf72871a",
=======
   "id": "8f32e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def insert_trajectory(\n",
    "    database,\n",
    "    item_id: str,\n",
    "    camera_id: str,\n",
    "    object_type: str,\n",
    "    postgres_timestamps: List[str],\n",
    "    pairs: List[Tuple[float, float, float]],\n",
    "    itemHeading_list: List[int],\n",
    "    translation_list: List[Tuple[float, float, float]],\n",
    "    road_types: List[str],\n",
    "    roadpolygon_list: List[List[Tuple[float, float]]]\n",
    "):\n",
    "    ### Save camera config\n",
    "    PICKLE_DATA_PATH = '/data/apperception-data/processed/nuscenes/full-dataset-v1.0/Mini/videos/boston-seaport'\n",
    "    import_pickle(database, PICKLE_DATA_PATH)\n",
    "    traj_centroids = []\n",
    "    translations = []\n",
    "    itemHeadings = []\n",
    "    roadTypes = []\n",
    "    roadPolygons = []\n",
    "    prevTimestamp = None\n",
    "    for timestamp, current_point, curItemHeading, current_trans, cur_road_type, cur_roadpolygon in zip(\n",
    "        postgres_timestamps, pairs, itemHeading_list, translation_list, road_types, roadpolygon_list\n",
    "    ):\n",
    "        if prevTimestamp == timestamp:\n",
    "            continue\n",
    "        prevTimestamp = timestamp\n",
    "\n",
    "        # Construct trajectory\n",
    "        traj_centroids.append(f\"POINT Z ({join(current_point, ' ')})@{timestamp}\")\n",
    "        translations.append(f\"POINT Z ({join(current_trans, ' ')})@{timestamp}\")\n",
    "        itemHeadings.append(f\"{curItemHeading}@{timestamp}\")\n",
    "        roadTypes.append(f\"{cur_road_type}@{timestamp}\")\n",
    "#         polygon_point = ', '.join(join(cur_point, ' ') for cur_point in list(\n",
    "#             zip(*cur_roadpolygon.exterior.coords.xy)))\n",
    "#         roadPolygons.append(f\"Polygon (({polygon_point}))@{timestamp}\")\n",
    "\n",
    "    # Insert the item_trajectory separately\n",
    "    insert_trajectory = f\"\"\"\n",
    "    INSERT INTO Item_General_Trajectory (itemId, cameraId, objectType, roadTypes, trajCentroids,\n",
    "    translations, itemHeadings)\n",
    "    VALUES (\n",
    "        '{item_id}',\n",
    "        '{camera_id}',\n",
    "        '{object_type}',\n",
    "        ttext '{{[{', '.join(roadTypes)}]}}',\n",
    "        tgeompoint '{{[{', '.join(traj_centroids)}]}}',\n",
    "        tgeompoint '{{[{', '.join(translations)}]}}',\n",
    "        tfloat '{{[{', '.join(itemHeadings)}]}}'\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    database.execute(insert_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982adb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_trajectory(obj_id, track):\n",
    "    timestamps: List[str] = []\n",
    "    pairs: List[Tuple[float, float, float]] = []\n",
    "    itemHeadings: List[int] = []\n",
    "    translations: List[Tuple[float, float, float]] = []\n",
    "    road_types: List[str] = []\n",
    "    roadpolygons: List[List[Tuple[float, float]]] = []\n",
    "\n",
    "    for tracking_result_2d, detection_info in track:\n",
    "        if detection_info:\n",
    "            camera_id = detection_info.ego_config.camera_id\n",
    "            object_type = tracking_result_2d.object_type\n",
    "            timestamps.append(detection_info.timestamp)\n",
    "            pairs.append(detection_info.car_loc3d)\n",
    "            itemHeadings.append(detection_info.segment_heading)\n",
    "            translations.append(detection_info.ego_config.ego_translation)\n",
    "            road_types.append(detection_info.road_type)\n",
    "            roadpolygons.append(detection_info.road_polygon_info.polygon)\n",
    "    print([obj_id, camera_id, object_type, timestamps, pairs,\n",
    "            itemHeadings, translations, road_types, roadpolygons])\n",
    "    return [obj_id, camera_id, object_type, timestamps, pairs,\n",
    "            itemHeadings, translations, road_types, roadpolygons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline(frames, pipeline, insert=False):\n",
    "    output = pipeline.run(Payload(frames)).__dict__\n",
    "    metadata = output['metadata']\n",
    "    kept_fn = [i for i, val in enumerate(output['keep']) if val]\n",
    "\n",
    "    detection_estimation_meta = metadata['DetectionEstimation']\n",
    "    sortmeta = metadata['Tracking2D.StrongSORT']\n",
    "    tracks = get_tracks(sortmeta, detection_estimation_meta)\n",
    "    for obj_id, track in tracks.items():\n",
    "        if insert:\n",
    "            insert_trajectory(database, *format_trajectory(obj_id, track))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9724cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(world):\n",
    "    pipeline = construct_pipeline(world)\n",
    "    if NUSCENES_PROCESSED_DATA in os.environ:\n",
    "        DATA_DIR = os.environ[NUSCENES_PROCESSED_DATA]\n",
    "    else:\n",
    "        DATA_DIR = \"/work/apperception/data/nuScenes/full-dataset-v1.0/Mini\"\n",
    "    with open(os.path.join(DATA_DIR, \"videos/boston-seaport\", \"frames.pickle\"), \"rb\") as f:\n",
    "        videos = pickle.load(f)\n",
    "\n",
    "    num_video = 0\n",
    "    for name, video in videos.items():\n",
    "        if name not in BOSTON_VIDEOS:\n",
    "            continue\n",
    "    #     if not name.endswith('CAM_FRONT'):\n",
    "    #         continue\n",
    "    #     if 'CAM_FRONT' not in name:\n",
    "    #         continue\n",
    "\n",
    "        print(name, '--------------------------------------------------------------------------------')\n",
    "        frames = Video(\n",
    "            os.path.join(DATA_DIR, \"videos/boston-seaport\", video[\"filename\"]),\n",
    "            [camera_config(*f, 0) for f in video[\"frames\"]],\n",
    "            video[\"start\"],\n",
    "        )\n",
    "        process_pipeline(frames, pipeline)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c97b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_time_camId_filename = world.get_id_time_camId_filename(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5d512",
>>>>>>> optimized_ingestion
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
<<<<<<< HEAD
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
=======
>>>>>>> optimized_ingestion
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
