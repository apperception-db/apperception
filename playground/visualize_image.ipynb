{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "import os\n",
    "import time\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "\n",
    "from apperception.utils import transformation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_camera_config(filename: str, database):\n",
    "    query = f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION ST_XYZ (g geometry) RETURNS real[] AS $$\n",
    "        BEGIN\n",
    "            RETURN ARRAY[ST_X(g), ST_Y(g), ST_Z(g)];\n",
    "        END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "\n",
    "    SELECT\n",
    "        cameraId,\n",
    "        ST_XYZ(egoTranslation),\n",
    "        egoRotation,\n",
    "        ST_XYZ(cameraTranslation),\n",
    "        cameraRotation,\n",
    "        cameraIntrinsic,\n",
    "        frameNum,\n",
    "        fileName,\n",
    "        cameraHeading,\n",
    "        egoHeading,\n",
    "        timestamp\n",
    "    FROM Cameras\n",
    "    WHERE\n",
    "        fileName = '{filename}'\n",
    "    ORDER BY cameraId ASC, frameNum ASC;\n",
    "    \"\"\"\n",
    "    result = database.execute(query)[0]\n",
    "    print(result)\n",
    "    camera_config = {\n",
    "        \"cameraId\": result[0],\n",
    "        \"egoTranslation\": result[1],\n",
    "        \"egoRotation\": result[2],\n",
    "        \"cameraTranslation\": result[3],\n",
    "        \"cameraRotation\": result[4],\n",
    "        \"cameraIntrinsic\": result[5],\n",
    "        \"frameNum\": result[6],\n",
    "        \"fileName\": result[7],\n",
    "        \"cameraHeading\": result[8],\n",
    "        \"egoHeading\": result[9],\n",
    "        \"timestamp\": result[10],\n",
    "    }\n",
    "    return camera_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_trajectory(itemId: str, time: str, database):\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION ST_XYZ (g geometry) RETURNS real[] AS $$\n",
    "        BEGIN\n",
    "            RETURN ARRAY[ST_X(g), ST_Y(g), ST_Z(g)];\n",
    "        END;\n",
    "        $$ LANGUAGE plpgsql;\n",
    "\n",
    "        SELECT ST_XYZ(valueAtTimestamp(trajCentroids, '{time}'))\n",
    "        FROM Item_General_Trajectory as final\n",
    "        WHERE itemId = '{itemId}';\n",
    "        \"\"\"\n",
    "\n",
    "    traj = database.execute(query)[0][0]\n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_to_pixel(\n",
    "    camera_config: dict, world_coords: Union[np.ndarray, Tuple[float, float, float]]\n",
    "):\n",
    "    traj_2d = transformation(world_coords, camera_config)\n",
    "    return traj_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_objects(frame, itemIds: Set[str], camera_config, database):\n",
    "    time = camera_config[\"timestamp\"]\n",
    "    pixels = {}\n",
    "    for itemId in itemIds:\n",
    "        current_traj_point = fetch_trajectory(itemId=itemId, time=time, database=database)\n",
    "\n",
    "        if None not in current_traj_point:\n",
    "            print(current_traj_point)\n",
    "            current_pixel = world_to_pixel(camera_config, current_traj_point)\n",
    "            print(current_pixel)\n",
    "            pixels[itemId] = current_pixel\n",
    "\n",
    "    for itemId in pixels:\n",
    "        pixel = pixels[itemId]\n",
    "        cv2.circle(\n",
    "            frame,\n",
    "            tuple([int(pixel[0][0]), int(pixel[1][0])]),\n",
    "            10,\n",
    "            (0, 255, 0),\n",
    "            -1,\n",
    "        )\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_bboxes(frame, bboxes):\n",
    "    for i in range(len(bboxes.detections)):\n",
    "        box = bboxes.detections[i]\n",
    "        bbox_left, bbox_top, bbox_right, bbox_buttom, _, _  = [int(x) for x in box]\n",
    "        cv2.rectangle(frame, (bbox_left, bbox_top), (bbox_right, bbox_buttom), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, \"c\" + str(1 + i), (bbox_left, bbox_buttom), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from yolo_tracker.yolov5.utils.augmentations import letterbox\n",
    "from yolo_tracker.yolov5.utils.general import (check_img_size,\n",
    "                                               non_max_suppression,\n",
    "                                               scale_boxes)\n",
    "from yolo_tracker.yolov5.utils.torch_utils import select_device\n",
    "\n",
    "from optimized_ingestion.stages.detection_2d.detection_2d import Metadatum\n",
    "\n",
    "def class_mapping_to_list(names: \"dict[int, str]\") -> \"list[str]\":\n",
    "    out: \"list[str]\" = []\n",
    "    for i, (idx, name) in enumerate(sorted(names.items())):\n",
    "        assert i == idx, (i, idx)\n",
    "        out.append(name)\n",
    "\n",
    "    return out\n",
    "\n",
    "def detect_image(im0):\n",
    "    im = letterbox(im0, 640, stride=32, auto=True)[0]  # padded resize\n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)\n",
    "    \n",
    "    im = torch.from_numpy(im).to(select_device(\"\"))\n",
    "    im = im.half() if False else im.float()\n",
    "    im /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    \n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s').model.to(select_device(\"\"))\n",
    "\n",
    "    # Inference\n",
    "    pred = model(im, augment=False)\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(\n",
    "        pred,\n",
    "        0.25,\n",
    "        0.45,\n",
    "        None,\n",
    "        False,\n",
    "        max_det=1000\n",
    "    )\n",
    "\n",
    "    # Process detections\n",
    "    assert isinstance(pred, list), type(pred)\n",
    "    assert len(pred) == 1, len(pred)\n",
    "    det = pred[0]\n",
    "    assert isinstance(det, torch.Tensor), type(det)\n",
    "    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "    return Metadatum(det, class_mapping_to_list(model.names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"/work/apperception/data/raw/nuScenes/full-dataset-v1.0/Trainval/\"\n",
    "filename = \"samples/CAM_BACK_LEFT/n008-2018-08-30-15-16-55-0400__CAM_BACK_LEFT__1535657120147405.jpg\"\n",
    "# itemIds = ['scene-0757-CAM_FRONT_obj_44', 'scene-0757-CAM_FRONT_obj_23']\n",
    "frame_im = cv2.imread(prefix + filename)\n",
    "\n",
    "# camera_config = fetch_camera_config(filename, database)\n",
    "# print(camera_config)\n",
    "# overlayed = overlay_objects(frame_im, itemIds, camera_config, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = detect_image(frame_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed = overlay_bboxes(frame_im, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "samples/CAM_FRONT/n008-2018-07-26-12-13-50-0400__CAM_FRONT__1532621918662404.jpg\n",
    "\n",
    "\n",
    "SELECT itemId, objectType, c.egoHeading, roadDirection(c.egoTranslation, c.egoHeading), ST_Distance(c.egoTranslation, valueAtTimestamp(t.translations, c.timestamp)), valueAtTimestamp(t.itemHeadings, c.timestamp), roadDirection(valueAtTimestamp(t.translations, c.timestamp),  valueAtTimestamp(t.itemHeadings, c.timestamp)::real), ST_X(egoTranslation), ST_Y(egoTranslation) from Item_General_Trajectory AS t, Cameras AS c where t.cameraId = c.cameraId AND c.filename = 'samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532708642362404.jpg' AND ST_Distance(c.egoTranslation, valueAtTimestamp(t.translations, c.timestamp)) < 50 AND viewAngle(valueAtTimestamp(t.translations,timestamp),egoHeading,egoTranslation) < 35 AND contained(valuecontained(egoTranslation,'lane')),'intersection') AND t.objectType LIKE 'human.pedestrian.adult' AND contained(egoTranslation,'lane');\n",
    "\n",
    "SELECT itemId, objectType, c.egoHeading, roadDirection(c.egoTranslation, c.egoHeading), ST_Distance(c.egoTranslation, valueAtTimestamp(t.translations, c.timestamp)), valueAtTimestamp(t.itemHeadings, c.timestamp), roadDirection(valueAtTimestamp(t.translations, c.timestamp),  valueAtTimestamp(t.itemHeadings, c.timestamp)::real) from Item_General_Trajectory AS t, Cameras AS c where t.cameraId = c.cameraId AND c.filename = 'samples/CAM_FRONT/n008-2018-07-26-12-13-50-0400__CAM_FRONT__1532621918662404.jpg';\n",
    "\n",
    "SELECT itemId, objectType, c.egoHeading, roadDirection(c.egoTranslation, c.egoHeading), ST_Distance(c.egoTranslation, valueAtTimestamp(t.translations, c.timestamp)), valueAtTimestamp(t.itemHeadings, c.timestamp), roadDirection(valueAtTimestamp(t.translations, c.timestamp),  valueAtTimestamp(t.itemHeadings, c.timestamp)::real), ST_X(egoTranslation), ST_Y(egoTranslation), ST_X(valueAtTimestamp(t.translations, c.timestamp)), st_y(valueAtTimestamp(t.translations, c.timestamp)) from Item_General_Trajectory AS t, Cameras AS c where t.cameraId = c.cameraId AND c.filename = 'samples/CAM_FRONT/n008-2018-08-28-16-16-48-0400__CAM_FRONT__1535488231612404.jpg' AND ST_Distance(c.egoTranslation, valueAtTimestamp(t.translations, c.timestamp)) < 50 AND viewAngle(valueAtTimestamp(t.translations,timestamp),egoHeading,egoTranslation) < 35 AND t.objectType LIKE 'human.pedestrian.adult' AND contained(egoTranslation, 'lanegroup') AND angleExcluding(facingRelative((headingAtTimestamp(t.itemHeadings,timestamp))::real,egoHeading),-70,70) AND angleBetween(facingRelative(egoHeading,roadDirection(egoTranslation,egoHeading)),-15,15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apperception",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
