{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6bd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# other1 = Car on intersection,\n",
    "#             facing Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# other2 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# require abs(relative heading of other1 from other2) > 100 deg\n",
    "# require (distance from ego to intersectionRegion) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e70533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F\n",
    "\n",
    "### Constants ###\n",
    "SAMPLING_RATE = 2\n",
    "CAMERA_ID = \"scene-0757\"\n",
    "\n",
    "CAMERA_COLUMNS = [\n",
    "    \"cameraId\",\n",
    "    \"frameId\",\n",
    "    \"frameNum\",\n",
    "    \"filename\",\n",
    "    \"cameraTranslation\",\n",
    "    \"cameraRotation\",\n",
    "    \"cameraIntrinsic\",\n",
    "    \"egoTranslation\",\n",
    "    \"egoRotation\",\n",
    "    \"timestamp\",\n",
    "    \"cameraHeading\",\n",
    "    \"egoHeading\",\n",
    "    \"cameraTranslationAbs\"] #road_direction not included yet\n",
    "\n",
    "def convert_frame_to_map(frame):\n",
    "    map_frame = dict(zip(CAMERA_COLUMNS, frames[:12]))\n",
    "    return map_frame\n",
    "\n",
    "def transform_to_world(frame_coordinate, ego_translation, ego_rotation):\n",
    "    ### TODO: get world coordinates\n",
    "    return frame_coordinate\n",
    "\n",
    "def get_obj_trajectory(tracking_df, ego_config):\n",
    "    '''\n",
    "    returned object info is a dictionary that looks like this:\n",
    "    {object_id:{frame_idx:[], #need to use the frame idx of the video to get the camera config for each frame\n",
    "                trajectory:[]}\n",
    "    '''\n",
    "    obj_info = {}\n",
    "    grouped_trajectory = tracking_df.groupby(by=[\"object_id\"])\n",
    "    for name, group in grouped_trajectory:\n",
    "        obj_info[name] = {}\n",
    "        group.frame_idx = group.frame_idx.add(-1)\n",
    "        object_df = group[[\n",
    "            'frame_idx', 'object_id', 'object_type', 'bbox_left', 'bbox_top', 'bbox_w', 'bbox_h']]\n",
    "        object_df = object_df.reset_index(drop=True)\n",
    "        framenums = group.frame_idx.tolist()\n",
    "        \n",
    "        ### get ego_config for each framenum\n",
    "        transformation_config = ego_config.iloc[framenums]\n",
    "        transformation_config = transformation_config.reset_index(drop=True)\n",
    "        \n",
    "        object_with_ego = pd.concat([object_df, transformation_config], axis=1)\n",
    "        ### for each coordinate, transform\n",
    "        obj_trajectory = []\n",
    "        for index, row in object_with_ego.iterrows():\n",
    "            obj_trajectory.append(transform_to_world(frame_coordinate=(row['bbox_left']+row['bbox_w']//2, \n",
    "                                                                        row['bbox_top']+row['bbox_h']//2),  \n",
    "                               ego_translation=row['egoTranslation'],\n",
    "                               ego_rotation=row['egoRotation']))\n",
    "        \n",
    "        obj_info[name]['frame_idx'] = object_with_ego[['frame_idx']]\n",
    "        obj_info[name]['trajectory'] = obj_trajectory\n",
    "    return obj_info\n",
    "\n",
    "\n",
    "def facing_relative(prev_traj_point, next_traj_point, current_ego_heading):\n",
    "    ### TODO: get direction from adjacent traj points, then calculate the relative degree\n",
    "    ####### COMPLETE\n",
    "    diff = next_traj_point - prev_traj_point\n",
    "    diff_heading = math.degrees(np.arctan2(diff[1], diff[0])) - 90\n",
    "    result = ((diff_heading - current_ego_heading) % 360 + 360) % 360\n",
    "    return result\n",
    "\n",
    "def facing_relative_check(obj_info, threshold):\n",
    "    for obj_id in obj_info:\n",
    "        frame_idx = obj_info[obj_id]['frame_idx'].frame_idx\n",
    "        trajectory = obj_info[obj_id]['trajectory']\n",
    "        ego_heading = ego_config.iloc[frame_idx.tolist()].egoHeading.tolist()\n",
    "        print(frame_idx[:len(ego_heading)-1][[facing_relative(trajectory[i], trajectory[i+1], ego_heading[i]) > threshold for i in range(len(ego_heading)-1)]])\n",
    "\n",
    "\"\"\"\n",
    "Dummy Modules for playing around\n",
    "\"\"\"\n",
    "class optimizeRoadNetwork:\n",
    "    def __init__(self, road_network=None):\n",
    "        self.road_network = road_network\n",
    "        self.optimized_road_network = self.optimize_road_network()\n",
    "\n",
    "    def optimize_road_network(self):\n",
    "        return self.road_network\n",
    "\n",
    "    def optimize_filter_intersection(self, sample_frames):\n",
    "        intersection_filtered = []\n",
    "\n",
    "        # TODO: Connection to DB for each execution might take too much time, do all at same time\n",
    "        for frame in sampled_frames:\n",
    "            map_frame = convert_frame_to_map(frame)\n",
    "            # use sql in order to make use of mobilitydb features. TODO: Find python alternative\n",
    "            query = f\"SELECT TRUE WHERE minDistance('{map_frame['egoTranslation']}', 'intersection') < 10\" \n",
    "            result = database._execute_query(query)\n",
    "            if result:\n",
    "                intersection_filtered.append(frame)\n",
    "\n",
    "        return intersection_filtered\n",
    "\n",
    "\n",
    "class optimizeSampling:\n",
    "    def __init__(self, sampling=None):\n",
    "        self.sampling = sampling\n",
    "        self.optimized_sampling = self.optimize_sampling()\n",
    "\n",
    "    def optimize_sampling(self):\n",
    "        return self.sampling\n",
    "    \n",
    "    def native_sample(self):\n",
    "        # Sample All Frames from Video at a #\n",
    "        query = f\"SELECT * FROM Cameras WHERE cameraId = '{CAMERA_ID}' ORDER BY frameNum\"\n",
    "\n",
    "        all_frames = database._execute_query(query)\n",
    "        sampled_frames = all_frames[::SAMPLING_RATE]\n",
    "        return sampled_frames\n",
    "\n",
    "\n",
    "class optimizeDecoding:\n",
    "    def __init__(self, decoding=None):\n",
    "        self.decoding = decoding\n",
    "        self.optimized_decoding = self.optimize_decoding()\n",
    "\n",
    "    def optimize_decoding(self):\n",
    "        return self.decoding\n",
    "\n",
    "    def optimize_tracking(self, lst_of_frames):\n",
    "        # Now the result is written to a txt file, need to fix this later\n",
    "        from Yolov5_Strong_Detection import sample_frame_tracker\n",
    "        for frames in lst_of_frames:\n",
    "            result = sample_frame_tracker.run(frames, save_vid=True)\n",
    "\n",
    "\"\"\"\n",
    "End to End Optimization Ingestion\n",
    "Use Case: select cars appears in intersection\n",
    "                      facing degree d relative to ego\n",
    "Test Data: only use one video as the test data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class optimizeIngestion:\n",
    "    def __init__(self):\n",
    "        self.optimize_road_network = optimizeRoadNetwork()\n",
    "        self.optimize_sampling = optimizeSampling()\n",
    "        self.optimize_decoding = optimizeDecoding()\n",
    "\n",
    "    def run_test(self):\n",
    "        # 1. Get all frames from video\n",
    "        all_frames = self.optimize_sampling.native_sample()\n",
    "        \n",
    "        \n",
    "        # 2. Filter out frames that in intersection\n",
    "        intersection_filtered = self.optimize_road_network.optimize_filter_intersection(all_frames)\n",
    "        ###TODO:fetch the camera_config corresponding to intersection_filtered \n",
    "        query = \"\"\"SELECT * FROM Cameras \n",
    "                    WHERE filename like 'samples/CAM_FRONT/%2018-08-01-15%' \n",
    "                    ORDER BY frameNum\"\"\" \n",
    "        camera_config = database._execute_query(query)\n",
    "        camera_config_df = pd.DataFrame(camera_config, columns=camera_columns)\n",
    "        ego_config = camera_config_df[['egoTranslation', 'egoRotation', 'egoHeading']]\n",
    "        \n",
    "        \n",
    "        # 3. Decode filtered_frames and track\n",
    "        self.optimize_decoding.optimize_tracking([intersection_filtered])\n",
    "        df = pd.read_csv(\"../optimization_playground/tracks/CAM_FRONT.txt\", sep=\" \", header=None, \n",
    "                 names=[\"frame_idx\", \n",
    "                        \"object_id\", \n",
    "                        \"bbox_left\", \n",
    "                        \"bbox_top\", \n",
    "                        \"bbox_w\", \n",
    "                        \"bbox_h\", \n",
    "                        \"None1\",\n",
    "                        \"None2\",\n",
    "                        \"None3\",\n",
    "                        \"None\",\n",
    "                        \"object_type\"])\n",
    "        \n",
    "        obj_info = get_obj_trajectory(df)\n",
    "        facing_relative_check(obj_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f44df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../Yolov5_StrongSORT_OSNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = sorted(glob.glob('/home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64558171",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames = [f for f in files if '2018-08-01-15' in f]\n",
    "len(test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627076c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sample_frame_tracker\n",
    "def track_sample(lst_of_frames):\n",
    "    for frames in lst_of_frames:\n",
    "        result = sample_frame_tracker.run(frames, save_vid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb82553",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_sample([test_frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../optimization_playground/tracks/CAM_FRONT.txt\", sep=\" \", header=None, \n",
    "                 names=[\"frame_idx\", \n",
    "                        \"object_id\", \n",
    "                        \"bbox_left\", \n",
    "                        \"bbox_top\", \n",
    "                        \"bbox_w\", \n",
    "                        \"bbox_h\", \n",
    "                        \"None1\",\n",
    "                        \"None2\",\n",
    "                        \"None3\",\n",
    "                        \"None\",\n",
    "                        \"object_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b41193",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_columns = [\n",
    "    \"cameraId\",\n",
    "    \"frameId\",\n",
    "    \"frameNum\",\n",
    "    \"filename\",\n",
    "    \"cameraTranslation\",\n",
    "    \"cameraRotation\",\n",
    "    \"cameraIntrinsic\",\n",
    "    \"egoTranslation\",\n",
    "    \"egoRotation\",\n",
    "    \"timestamp\",\n",
    "    \"cameraHeading\",\n",
    "    \"egoHeading\",\n",
    "    \"cameraTranslationAbs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample All Frames from Video at a #\n",
    "query = f\"SELECT * FROM Cameras WHERE filename like 'samples/CAM_FRONT/%2018-08-01-15%' ORDER BY frameNum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_config = database._execute_query(query)\n",
    "camera_config_df = pd.DataFrame(camera_config, columns=camera_columns)\n",
    "ego_config = camera_config_df[['egoTranslation', 'egoRotation', 'egoHeading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_world(frame_coordinate, ego_translation, ego_rotation):\n",
    "    ### TODO: get world coordinates\n",
    "    return frame_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_trajectory(tracking_df):\n",
    "    '''\n",
    "    returned object info is a dictionary that looks like this:\n",
    "    {object_id:{frame_idx:[], #need to use the frame idx of the video to get the camera config for each frame\n",
    "                trajectory:[]}\n",
    "    '''\n",
    "    obj_info = {}\n",
    "    grouped_trajectory = tracking_df.groupby(by=[\"object_id\"])\n",
    "    for name, group in grouped_trajectory:\n",
    "        obj_info[name] = {}\n",
    "        group.frame_idx = group.frame_idx.add(-1)\n",
    "        object_df = group[[\n",
    "            'frame_idx', 'object_id', 'object_type', 'bbox_left', 'bbox_top', 'bbox_w', 'bbox_h']]\n",
    "        object_df = object_df.reset_index(drop=True)\n",
    "        framenums = group.frame_idx.tolist()\n",
    "        \n",
    "        ### get ego_config for each framenum\n",
    "        transformation_config = ego_config.iloc[framenums]\n",
    "        transformation_config = transformation_config.reset_index(drop=True)\n",
    "        \n",
    "        object_with_ego = pd.concat([object_df, transformation_config], axis=1)\n",
    "        ### for each coordinate, transform\n",
    "        obj_trajectory = []\n",
    "        for index, row in object_with_ego.iterrows():\n",
    "            obj_trajectory.append(transform_to_world(frame_coordinate=((row['bbox_left'], row['bbox_top']), \n",
    "                                                 (row['bbox_left']+row['bbox_w'], \n",
    "                                                  row['bbox_top']+row['bbox_h'])), \n",
    "                               ego_translation=row['egoTranslation'],\n",
    "                               ego_rotation=row['egoRotation']))\n",
    "        \n",
    "        obj_info[name]['frame_idx'] = object_with_ego[['frame_idx']]\n",
    "        obj_info[name]['trajectory'] = obj_trajectory\n",
    "    return obj_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_info = get_obj_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facing_relative(prev_traj_point, next_traj_point, current_ego_heading):\n",
    "    ### TODO: get direction from adjacent traj points, then calculate the relative degree\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facing_relative_check(obj_info, threshold):\n",
    "    for obj_id in obj_info:\n",
    "        frame_idx = obj_info[obj_id]['frame_idx'].frame_idx\n",
    "        trajectory = obj_info[obj_id]['trajectory']\n",
    "        ego_heading = ego_config.iloc[frame_idx.tolist()].egoHeading.tolist()\n",
    "        print(frame_idx[:len(ego_heading)-1][[facing_relative(trajectory[i], trajectory[i+1], ego_heading[i]) > threshold for i in range(len(ego_heading)-1)]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "facing_relative_check(obj_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1778c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('apperception_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef986073a7322f2daa7cef2e5604e6018e5522cc159657af8e7aa863491a7631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
