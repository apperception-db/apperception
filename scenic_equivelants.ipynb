{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the .apperception_cache if it exists, as to avoid DB conflict errors\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dirpath = os.path.join('.apperception_cache')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "\n",
    "dirpath = os.path.join('output')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "os.mkdir(dirpath)\n",
    "\n",
    "# This piece of code is unsafe, and should not be run if not needed. \n",
    "# It serves for test purposes when one recieves a \"dead kernel\" error.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get backend Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"apperception\"))\n",
    "\n",
    "### IMPORTS\n",
    "import lens\n",
    "import point\n",
    "from new_world import empty_world\n",
    "\n",
    "# Let's define some attribute for constructing the world first\n",
    "name = \"trafficScene\"  # world name\n",
    "units = \"metrics\"  # world units\n",
    "video_file = \"amber_videos/traffic-scene.mp4\"  # example video file\n",
    "lens_attrs = {\"fov\": 120, \"cam_origin\": (0, 0, 0), \"skew_factor\": 0}\n",
    "point_attrs = {\"p_id\": \"p1\", \"cam_id\": \"cam1\", \"x\": 0, \"y\": 0, \"z\": 0, \"time\": None, \"type\": \"pos\"}\n",
    "camera_attrs = {\"ratio\": 0.5}\n",
    "fps = 30\n",
    "\n",
    "# 1. define a world\n",
    "traffic_world = empty_world(name)\n",
    "\n",
    "# 2. construct a camera\n",
    "fov, res, cam_origin, skew_factor = (\n",
    "    lens_attrs[\"fov\"],\n",
    "    [1280, 720],\n",
    "    lens_attrs[\"cam_origin\"],\n",
    "    lens_attrs[\"skew_factor\"],\n",
    ")\n",
    "cam_lens = lens.PinholeLens(res, cam_origin, fov, skew_factor)\n",
    "\n",
    "pt_id, cam_id, x, y, z, time, pt_type = (\n",
    "    point_attrs[\"p_id\"],\n",
    "    point_attrs[\"cam_id\"],\n",
    "    point_attrs[\"x\"],\n",
    "    point_attrs[\"y\"],\n",
    "    point_attrs[\"z\"],\n",
    "    point_attrs[\"time\"],\n",
    "    point_attrs[\"type\"],\n",
    ")\n",
    "location = point.Point(pt_id, cam_id, (x, y, z), time, pt_type)\n",
    "\n",
    "ratio = camera_attrs[\"ratio\"]\n",
    "\n",
    "# ingest the camera into the world\n",
    "traffic_world = traffic_world.add_camera(\n",
    "    cam_id=cam_id,\n",
    "    location=location,\n",
    "    ratio=ratio,\n",
    "    video_file=video_file,\n",
    "    metadata_identifier=name + \"_\" + cam_id,\n",
    "    lens=cam_lens,\n",
    ")\n",
    "\n",
    "# Call execute on the world to run the detection algorithm and save the real data to the database\n",
    "recognized_world = traffic_world.recognize(cam_id)\n",
    "\n",
    "volume = traffic_world.select_intersection_of_interest_or_use_default(cam_id=cam_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras are [('cam1', 0.5, 0.0, 0.0, 0.0, 369.5041722813606, 207.84609690826534, 120, 0.0)]\n",
      "lens are [(0.5, 0.0, 0.0, 0.0, 120, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "cams = traffic_world.get_camera()\n",
    "lens = traffic_world.get_len()\n",
    "# ids = traffic_world.get_id()\n",
    "print(\"cameras are\", cams)\n",
    "print(\"lens are\", lens)\n",
    "# print(\"ids are\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights from C:\\Users\\youse\\Desktop\\Research\\Apperception\\apperception\\apperception\\../yolov5-deepsort/deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7... Done!\n",
      "YOLOv5  v6.0-159-gdb6ec66 torch 1.10.2+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car-1 saved successfully\n",
      "person-2 saved successfully\n",
      "traffic light-3 saved successfully\n",
      "car-4 saved successfully\n",
      "person-5 saved successfully\n",
      "car-6 saved successfully\n",
      "traffic light-7 saved successfully\n",
      "person-8 saved successfully\n",
      "car-9 saved successfully\n",
      "person-10 saved successfully\n",
      "person-11 saved successfully\n",
      "person-14 saved successfully\n",
      "traffic light-14 saved successfully\n",
      "car-16 saved successfully\n",
      "traffic light-5 saved successfully\n",
      "person-17 saved successfully\n",
      "bus-16 saved successfully\n",
      "traffic light-17 saved successfully\n",
      "car-21 saved successfully\n",
      "truck-21 saved successfully\n",
      "bus-21 saved successfully\n",
      "person-22 saved successfully\n",
      "person-6 saved successfully\n",
      "person-25 saved successfully\n",
      "car-5 saved successfully\n",
      "traffic light-22 saved successfully\n",
      "car-27 saved successfully\n",
      "traffic light-11 saved successfully\n",
      "truck-29 saved successfully\n",
      "person-30 saved successfully\n",
      "person-16 saved successfully\n",
      "truck-1 saved successfully\n",
      "bus-1 saved successfully\n",
      "person-42 saved successfully\n",
      "person-45 saved successfully\n",
      "get_traj_key SELECT sq2.itemid FROM (SELECT DISTINCT sq1.* FROM (SELECT * FROM (SELECT * FROM item_general_trajectory WHERE worldId='a51f7975-379b-4df4-b4ce-7ba216f0b05e') sq0 WHERE sq0.objecttype='car') sq1 CROSS JOIN cameras WHERE ST_X(cameras.origin)-getX(sq1.trajCentroids)>=-10 AND ST_X(cameras.origin)-getX(sq1.trajCentroids)<=10 AND ST_Y(cameras.origin)-getX(sq1.trajCentroids)>=-1 AND ST_Y(cameras.origin)-getX(sq1.trajCentroids)<=5 AND ST_Z(cameras.origin)-getX(sq1.trajCentroids)>=-10 AND ST_Z(cameras.origin)-getX(sq1.trajCentroids)<=0) sq2\n",
      "filtered_ids are [('car-4-a51f7975-379b-4df4-b4ce-7ba216f0b05e',), ('car-6-a51f7975-379b-4df4-b4ce-7ba216f0b05e',), ('car-9-a51f7975-379b-4df4-b4ce-7ba216f0b05e',)]\n",
      "----------------------------------------------------------------------\n",
      "Total execution time is: 44.988890647888184 seconds\n",
      "Device Details: \n",
      " Processor: AMD Ryzen 7 5800H \n",
      " RAM Size: 16GB \n",
      " Graphics Card: NVIDIA GeForce RTX 3060 Laptop\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "### Scenic Code ###\n",
    "# ego = Car\n",
    "# Car offset by (Range(-10, 10), Range(20, 40))\n",
    "\n",
    "### Apperception Query ###\n",
    "filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "## OPTION 1 ###\n",
    "filtered_world = filtered_world.filter_relative_to_type(x_range=(-10, 10), y_range=(-1, 5), z_range=(-10, 0),\n",
    "                                                        type=\"camera\")\n",
    "# The idea is that the user passes in a lambda function, that specifies the relationship that must be met between the queried\n",
    "# object, and some object of the type passed to the function. In this case, the lambda function filters such that the offset \n",
    "# is between -10 and 10 in the x direction, and between 20 and 40 in the y direction, relative to some camera.\n",
    "\n",
    "### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=None, type=\"camera\")\n",
    "# The idea is that filter_offset_type() takes in two arguments: the offset in terms of coordinates, a relative heading \n",
    "# as well as the type of object to be offset from. In this case, we want it to be somehwere between -10 and 10 units\n",
    "# offset relative to a camera's x position, somehwere between 20 and 40 units offset relative to some camera's y position, \n",
    "# and we dont care about the offset relative to the camera's z position. We also dont care about the relative heading difference.\n",
    "\n",
    "filtered_ids = filtered_world.get_traj_key()\n",
    "print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Total execution time is: %s seconds\" % (time.time() - start_time))\n",
    "print(\"Device Details: \\n Processor: AMD Ryzen 7 5800H \\n RAM Size: 16GB \\n Graphics Card: NVIDIA GeForce RTX 3060 Laptop\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "# traffic-scene-shorter (length of 4 seconds): runtime of 81.82859063148499 seconds\n",
    "# traffic-scene (length of 20 seconds): runtime of 98.58345794677734 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 4)\n",
      "(88, 4)\n",
      "incorrect length: 88\n",
      "(24, 4)\n",
      "incorrect length: 24\n",
      "output video files ./output/trafficScene_cam1car-4-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4,./output/trafficScene_cam1car-6-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4,./output/trafficScene_cam1car-9-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4\n"
     ]
    }
   ],
   "source": [
    "filtered_world.get_video([cam_id], boxed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\").interval(0, fps * 3)\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "# # render tracking video\n",
    "# filtered_world.get_video([cam_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40,\n",
    "#                                                         type=\"camera\")\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=None, type=\"camera\")\n",
    "\n",
    "# filtered_world = filtered_world.filter_heading(-5, 5)\n",
    "# # Filters for objects that have heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40 \\\n",
    "#                                                                                   and -5 <= (camera.heading - obj.heading) <= 5,\n",
    "#                                                         type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=(-5, 5), type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car left of ego by 0.25 \n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def left_of(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y)\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=left_of, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # badAngle = Range(10, 20) deg\n",
    "# # Car left of ego by 0.25,\n",
    "# # \tfacing badAngle relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def filter(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y) and 10 <= (camera.heading - obj.heading) <= 20\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=filter, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roadDirection(x, y, z):\n",
    "#     # TODO: Implement\n",
    "#     # Returns the direction (in 360 degree angle form) of the road at the coordinates (x, y, z)\n",
    "#     # If their is no such road, returns a value of None\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FURTHER QUERIES WILL USE THE OPTION 1 LISTED ABOVE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# weather = Uniform(\"sunny\", \"rainy\", \"thunder\")\n",
    "# time = Range(10, 12)\n",
    "#\n",
    "# ego = car on road\n",
    "# otherCar = Car ahead of ego by Range(4, 19)\n",
    "# require not (otherCar in intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# spot = OrientedPoint on curb\n",
    "# ego = Car at (spot offset by (Range(2,4), Range(5,10)))\n",
    "# sideCar = Car left of spot by Range(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(car, numCars):\n",
    "#     for i in range(numCars):\n",
    "#         car = Car ahead of car by Range(4, 5)\n",
    "#         leftCar = Car left of car by Normal(2, 0.1), facing roadDirection\n",
    "#         rightCar = Car right of car by Normal(3, 0.1), facing Range(0, 10) deg relative to ego.heading\n",
    "#     return leftCar, rightCar\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point, with visible_distance 200\n",
    "\n",
    "# leftCar, rightCar = placeObjs(ego, 2)\n",
    "# require (distance to leftCar) < 200\n",
    "# require (distance to rightCar) < 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(numPeds):\n",
    "#     for i in range(numPeds):\n",
    "#         Pedestrian offset by Range(-5, 5) @ Range(0, 200),\n",
    "#             facing Range(-120, 120) deg relative to ego.heading\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point,\n",
    "#         with visibleDistance 200\n",
    "\n",
    "# placeObjs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50, \n",
    "#         with viewAngle 135 deg\n",
    "# ped = Pedestrian on roadsOrIntersections,\n",
    "#         with regionContainedIn roadRegion,\n",
    "#         facing Range(-180, 180) deg\n",
    "\n",
    "# require abs(relative heading of ped from ego) > 70 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# offset = Uniform(-1, 1) * Range(90, 180) deg\n",
    "\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing offset relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# otherCar = Car on visible road,\n",
    "#             facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# require (distance from ego to otherCar) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# other1 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# other2 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# require abs(relative heading of other1 from other2) > 100 deg\n",
    "# require (distance from ego to intersectionRegion) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# point1 = OrientedPoint ahead of ego by Range(0, 40)\n",
    "# Car at (point1 offset by Range(-1, 1) & 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# oppositeCar = Car offset by (Range(-10, -1), Range(0, 50)),\n",
    "#     facing Range(140, 180) deg relative to ego.heading\n",
    "\n",
    "# point2 = OrientedPoint ahead of oppositeCar by Range(0, 40)\n",
    "# Car at (point2 offset by Range(-1, 1) @ 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# lanesWithRightLane = filter(lambda i: i._laneToRight, network.laneSections)\n",
    "# egoLane = Uniform(*lanesWithRightLane)\n",
    "\n",
    "# ego = Car on egoLane,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection\n",
    "# cutInCar = Car offset by Range(0, 4) @ Range(0, 5),\n",
    "#             facing -1*Range(15, 30) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think there are 3 main things that need to now be implemented in Apperception to allow incorparation with Scenic:\n",
    "# 1. A way to have the arbitrary filters that were possible in the old API (the predicate lambda functions that could be passed)\n",
    "# 2. A way to have filters with regardes to other objects. For example, I could say I want \"cars that are to the left of a bus by 0.25m\" or such. I would assume this could also be implemented as a lambda function filter (I have included an example fo this in the scenic_equivelants notebook).\n",
    "# 3. Some way to not only recognize what the type of an object is, but recognize the type of point it is on. For example, recognizing that the Car is on a road, or that the Car is in an intersection (this is something that is done quite a lot in Scenic).\n",
    "#      - For this, we might not have to incorporate it into apperception, and can make it the users responsibility (and they can create their own filters that do this), but I am not too sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b9f45d2c0c5940d48526f9dac9a46c8afda5d718c8f108cd3f22cd85be16c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
