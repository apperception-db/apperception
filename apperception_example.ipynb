{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"apperception\"))\n",
    "\n",
    "### IMPORTS\n",
    "import cv2\n",
    "\n",
    "from world import *\n",
    "from world_executor import *\n",
    "from video_util import *\n",
    "from metadata_util import *\n",
    "import lens\n",
    "\n",
    "import psycopg2\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/yongming/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/yongming/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-> Loading model from  models/mono+stereo_640x192\n",
      "   Loading pretrained encoder\n",
      "   Loading pretrained decoder\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### Let's define some attribute for constructing the world first\n",
    "name = 'traffic_scene' # world name\n",
    "units = 'metrics'      # world units\n",
    "cam1_id = 'cam1'\n",
    "cam1_video_file = \"../visual_road_video_cut/traffic-001_clip_clip.mp4\" #cam1 view\n",
    "cam2_id = 'cam2'\n",
    "cam2_video_file = \"../visual_road_video_cut/traffic-002_clip_clip.mp4\" #cam2 view"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### First we define a world\n",
    "traffic_world = World(name=name, units=units)\n",
    "\n",
    "### Use TASM if it's available on the machine\n",
    "# traffic_world = World(name=name, units=units, enable_tasm=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "### Secondly we construct the camera\n",
    "cam1_len = lens.VRLens(resolution=[3840, 2160], cam_origin=(199, -239, 5.2), yaw=-52, roll=0, pitch=0, field_of_view=90)\n",
    "cam2_len = lens.VRLens(resolution=[3840, 2160], cam_origin=(210, -270, 9), yaw=128, roll=0, pitch=-30, field_of_view=90)\n",
    "fps = 30"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "focal_width 1919.9999999999998\n",
      "focal_height 1079.9999999999998\n",
      "scaling matrix is [[ 5.20833333e-04  0.00000000e+00 -1.00000000e+00]\n",
      " [ 0.00000000e+00  9.25925926e-04 -1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "focal_width 1919.9999999999998\n",
      "focal_height 1079.9999999999998\n",
      "scaling matrix is [[ 5.20833333e-04  0.00000000e+00 -1.00000000e+00]\n",
      " [ 0.00000000e+00  9.25925926e-04 -1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "### Ingest the camera to the world\n",
    "traffic_world = traffic_world.camera(cam_id=cam1_id, \n",
    "                               video_file=cam1_video_file, \n",
    "                               metadata_identifier=name+\"_\"+cam1_id, \n",
    "                               lens=cam1_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Call execute on the world to run the detection algorithm and save the real data to the database\n",
    "recognized_world = traffic_world.recognize(cam1_id)\n",
    "recognized_world.execute()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Worlds Table created successfully........\n",
      "New world inserted successfully........\n",
      "Camera Table created successfully........\n",
      "New camera inserted successfully.........\n",
      "# of tracked items: 15\n",
      "car-1 saved successfully\n",
      "car-2 saved successfully\n",
      "person-3 saved successfully\n",
      "traffic light-4 saved successfully\n",
      "traffic light-5 saved successfully\n",
      "person-6 saved successfully\n",
      "person-7 saved successfully\n",
      "person-8 saved successfully\n",
      "person-10 saved successfully\n",
      "car-14 saved successfully\n",
      "person-18 saved successfully\n",
      "car-19 saved successfully\n",
      "person-28 saved successfully\n",
      "car-29 saved successfully\n",
      "person-31 saved successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(np.load(\"./left_top_1.npy\"))\n",
    "np.load(\"./left_top_2.npy\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 198.22919408  198.23179199  198.23252163  198.23689946  198.23730845\n",
      "   198.23949736  198.24031533  198.24104497  198.24063599  198.24063599\n",
      "   198.23990635  198.23698779  198.23625815  198.23625815  198.23739677\n",
      "   198.2374851   198.23789408  198.23342792  198.23164798]\n",
      " [-239.62714181 -239.62983246 -239.63040252 -239.63382286 -239.63480334\n",
      "  -239.63651351 -239.63847447 -239.63904453 -239.63806405 -239.63806405\n",
      "  -239.63749399 -239.63521376 -239.63464371 -239.63464371 -239.63619424\n",
      "  -239.63758514 -239.63856562 -239.63375438 -239.63302469]\n",
      " [   6.2           6.2           6.2           6.2           6.2\n",
      "     6.2           6.2           6.2           6.2           6.2\n",
      "     6.2           6.2           6.2           6.2           6.2\n",
      "     6.2           6.2           6.2           6.2       ]\n",
      " [   1.            1.            1.            1.            1.\n",
      "     1.            1.            1.            1.            1.\n",
      "     1.            1.            1.            1.            1.\n",
      "     1.            1.            1.            1.        ]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 209.49711409,  209.49711409],\n",
       "       [-269.55931449, -269.55931449],\n",
       "       [  10.17644207,   10.17644207],\n",
       "       [   1.        ,    1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "traffic_world = traffic_world.camera(cam_id=cam2_id, \n",
    "                               video_file=cam2_video_file, \n",
    "                               metadata_identifier=name+\"_\"+cam2_id, \n",
    "                               lens=cam2_len)\n",
    "tracking_results = recognize(cam2_video_file)\n",
    "conn = psycopg2.connect(database=\"mobilitydb\", user=\"docker\", password=\"docker\", host=\"localhost\", port=25432)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yongming/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of tracked items: 9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "add_recognized_objs(conn, cam2_len, tracking_results, traffic_world.VideoContext.start_time, temp=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "car-1 saved successfully\n",
      "car-2 saved successfully\n",
      "person-3 saved successfully\n",
      "traffic light-4 saved successfully\n",
      "car-5 saved successfully\n",
      "car-6 saved successfully\n",
      "person-8 saved successfully\n",
      "car-10 saved successfully\n",
      "person-11 saved successfully\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "volume = traffic_world.select_intersection_of_interest_or_use_default(cam_id=cam1_id)\n",
    "filtered_world = traffic_world.predicate(lambda obj:obj.object_type == \"car\")\n",
    "# filtered_world = traffic_world.predicate(lambda obj:obj.object_type == \"car\").predicate(lambda obj:obj.location in volume, {\"volume\":volume})\n",
    "# filtered_world = filtered_world.interval([0,fps*4])\n",
    " \n",
    "### to get the trajectory and the video over the entire trajectory(amber case)\n",
    "filtered_ids = filtered_world.selectkey(distinct = True).execute()\n",
    "print(\"filtered_ids are\", filtered_ids)\n",
    "print(len(filtered_ids))\n",
    "if len(filtered_ids)>0:\n",
    "    id_array = [e[0] for e in filtered_ids]\n",
    "    ### Fetch the trajectory of these items\n",
    "    trajectory = traffic_world.predicate(lambda obj: obj.object_id in id_array, {\"id_array\":id_array}).get_trajectory(distinct=True).execute()\n",
    "    traffic_world.overlay_trajectory(cam1_id, trajectory)\n",
    "    ### Get the videos of these items\n",
    "    entire_video = traffic_world.predicate(lambda obj: obj.object_id in id_array, {\"id_array\":id_array}).get_video()\n",
    "    entire_video.execute()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'cam1': <video_context.Camera object at 0x7f84024c3c50>, 'cam2': <video_context.Camera object at 0x7f84024c3eb8>}\n",
      "Module(body=[Return(value=Compare(left=Attribute(value=Name(id='obj', ctx=Load()), attr='object_type', ctx=Load()), ops=[Eq()], comparators=[Str(s='car')]))])\n",
      "Worlds Table created successfully........\n",
      "New world inserted successfully........\n",
      "Camera Table created successfully........\n",
      "New camera inserted successfully.........\n",
      "Camera Table created successfully........\n",
      "New camera inserted successfully.........\n",
      "SELECT distinct on(itemId) Item_General_Trajectory.itemId From Item_General_Trajectory  Where Item_General_Trajectory.objectType='car';\n",
      "\n",
      "filtered_ids are [['car-1']\n",
      " ['car-14']\n",
      " ['car-19']\n",
      " ['car-2']\n",
      " ['car-29']]\n",
      "5\n",
      "Module(body=[Return(value=Compare(left=Attribute(value=Name(id='obj', ctx=Load()), attr='object_id', ctx=Load()), ops=[In()], comparators=[Name(id='id_array', ctx=Load())]))])\n",
      "Worlds Table created successfully........\n",
      "New world inserted successfully........\n",
      "Camera Table created successfully........\n",
      "New camera inserted successfully.........\n",
      "Camera Table created successfully........\n",
      "New camera inserted successfully.........\n",
      "atPeriodSet(Item_General_Trajectory.trajCentroids,'{[0001-01-01 00:00:00, 9999-12-31 23:59:59.999999)}')\n",
      "asMFJSON(atPeriodSet(Item_General_Trajectory.trajCentroids,'{[0001-01-01 00:00:00, 9999-12-31 23:59:59.999999)}'))::json->'coordinates'\n",
      "SELECT distinct on(itemId) asMFJSON(atPeriodSet(Item_General_Trajectory.trajCentroids,'{[0001-01-01 00:00:00, 9999-12-31 23:59:59.999999)}'))::json->'coordinates' From Item_General_Trajectory  Where Item_General_Trajectory.itemId IN ('car-1','car-14','car-19','car-2','car-29');\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1020612e58a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m### Fetch the trajectory of these items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraffic_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id_array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mid_array\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtraffic_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlay_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam1_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m### Get the videos of these items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mentire_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraffic_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id_array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mid_array\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/research/apperception/apperception/world.py\u001b[0m in \u001b[0;36moverlay_trajectory\u001b[0;34m(self, cam_id, trajectory)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcurrent_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mframe_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_to_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_trajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/research/apperception/apperception/lens.py\u001b[0m in \u001b[0;36mworld_to_pixels\u001b[0;34m(self, world_coords)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mTranslate\u001b[0m \u001b[0mworld\u001b[0m \u001b[0mcoordinates\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \t\t\"\"\"\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mscaled_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_rotational_transform\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mworld_coords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0moriginal_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_scaling_transform\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscaled_pixels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_pixels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_pixels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}