{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the .apperception_cache if it exists, as to avoid DB conflict errors\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dirpath = os.path.join('.apperception_cache')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "\n",
    "dirpath = os.path.join('output')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "os.mkdir(dirpath)\n",
    "\n",
    "# This piece of code is unsafe, and should not be run if not needed. \n",
    "# It serves for test purposes when one recieves a \"dead kernel\" error.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.apperception_cache/2022-03-04 07;30;49.195866_b0836101-4935-450b-a2e0-22f469cf8994_trafficScene.ap.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/04mdk5z529z3kdnl2cl6ty3h0000gn/T/ipykernel_22556/3245952245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 1. define a world\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtraffic_world\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_world\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 2. construct a camera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_world.py\u001b[0m in \u001b[0;36mempty_world\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_empty_world_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_empty_world\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_world.py\u001b[0m in \u001b[0;36m_empty_world\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.apperception_cache/2022-03-04 07;30;49.195866_b0836101-4935-450b-a2e0-22f469cf8994_trafficScene.ap.yaml'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"apperception\"))\n",
    "\n",
    "### IMPORTS\n",
    "import lens\n",
    "import point\n",
    "from new_world import empty_world\n",
    "\n",
    "# Let's define some attribute for constructing the world first\n",
    "name = \"trafficScene\"  # world name\n",
    "units = \"metrics\"  # world units\n",
    "video_file = \"amber_videos/traffic-scene.mp4\"  # example video file\n",
    "lens_attrs = {\"fov\": 120, \"cam_origin\": (0, 0, 0), \"skew_factor\": 0}\n",
    "point_attrs = {\"p_id\": \"p1\", \"cam_id\": \"cam1\", \"x\": 0, \"y\": 0, \"z\": 0, \"time\": None, \"type\": \"pos\"}\n",
    "camera_attrs = {\"ratio\": 0.5}\n",
    "fps = 30\n",
    "\n",
    "# 1. define a world\n",
    "traffic_world = empty_world(name)\n",
    "\n",
    "# 2. construct a camera\n",
    "fov, res, cam_origin, skew_factor = (\n",
    "    lens_attrs[\"fov\"],\n",
    "    [1280, 720],\n",
    "    lens_attrs[\"cam_origin\"],\n",
    "    lens_attrs[\"skew_factor\"],\n",
    ")\n",
    "cam_lens = lens.PinholeLens(res, cam_origin, fov, skew_factor)\n",
    "\n",
    "pt_id, cam_id, x, y, z, time, pt_type = (\n",
    "    point_attrs[\"p_id\"],\n",
    "    point_attrs[\"cam_id\"],\n",
    "    point_attrs[\"x\"],\n",
    "    point_attrs[\"y\"],\n",
    "    point_attrs[\"z\"],\n",
    "    point_attrs[\"time\"],\n",
    "    point_attrs[\"type\"],\n",
    ")\n",
    "location = point.Point(pt_id, cam_id, (x, y, z), time, pt_type)\n",
    "\n",
    "ratio = camera_attrs[\"ratio\"]\n",
    "\n",
    "# ingest the camera into the world\n",
    "traffic_world = traffic_world.add_camera(\n",
    "    cam_id=cam_id,\n",
    "    location=location,\n",
    "    ratio=ratio,\n",
    "    video_file=video_file,\n",
    "    metadata_identifier=name + \"_\" + cam_id,\n",
    "    lens=cam_lens,\n",
    ")\n",
    "\n",
    "# Call execute on the world to run the detection algorithm and save the real data to the database\n",
    "recognized_world = traffic_world.recognize(cam_id)\n",
    "\n",
    "volume = traffic_world.select_intersection_of_interest_or_use_default(cam_id=cam_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cameras are [('cam1', 0.5, 0.0, 0.0, 0.0, 369.5041722813606, 207.84609690826534, 120, 0.0)]\n",
      "lens are [(0.5, 0.0, 0.0, 0.0, 120, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "cams = traffic_world.get_camera()\n",
    "lens = traffic_world.get_len()\n",
    "# ids = traffic_world.get_id()\n",
    "print(\"cameras are\", cams)\n",
    "print(\"lens are\", lens)\n",
    "# print(\"ids are\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_traj_key SELECT sq2.itemid FROM (SELECT DISTINCT sq1.* FROM (SELECT * FROM (SELECT * FROM item_general_trajectory WHERE worldId='cd5ca1e8-dc85-45bb-9a78-fe66a587ad06') sq0 WHERE sq0.objecttype='car') sq1 CROSS JOIN cameras WHERE ST_X(cameras.origin)-getX(sq1.trajCentroids)>=-10 AND ST_X(cameras.origin)-getX(sq1.trajCentroids)<=10 AND ST_Y(cameras.origin)-getX(sq1.trajCentroids)>=-15 AND ST_Y(cameras.origin)-getX(sq1.trajCentroids)<=70 AND ST_Z(cameras.origin)-getX(sq1.trajCentroids)>=-inf AND ST_Z(cameras.origin)-getX(sq1.trajCentroids)<=inf) sq2\n"
     ]
    },
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/04mdk5z529z3kdnl2cl6ty3h0000gn/T/ipykernel_22556/1961925502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# and we dont care about the offset relative to the camera's z position. We also dont care about the relative heading difference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfiltered_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_traj_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_ids are\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_world.py\u001b[0m in \u001b[0;36mget_traj_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_traj_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         return derive_world(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAJ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_world.py\u001b[0m in \u001b[0;36m_execute_from_root\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;31m# print(query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_world.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"world_id\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarkw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"world_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_world_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_print_lineage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/apperception/apperception/new_db.py\u001b[0m in \u001b[0;36mget_traj_key\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowflakeQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"itemid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get_traj_key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "### Scenic Code ###\n",
    "# ego = Car\n",
    "# Car offset by (Range(-10, 10), Range(20, 40))\n",
    "\n",
    "### Apperception Query ###\n",
    "filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(x_range=(-10, 10), y_range=(-1, 5), z_range=(-10, 0),\n",
    "#                                                         type=\"camera\")\n",
    "# The idea is that the user passes in a lambda function, that specifies the relationship that must be met between the queried\n",
    "# object, and some object of the type passed to the function. In this case, the lambda function filters such that the offset \n",
    "# is between -10 and 10 in the x direction, and between 20 and 40 in the y direction, relative to some camera.\n",
    "\n",
    "### OPTION 2 ###\n",
    "filtered_world = filtered_world.filter_pred_relative_to_type(pred = lambda obj: (cam.x - 10) <= obj.x <= (cam.x + 10) and (cam.y - 15) <= obj.y <= (cam.y + 70))\n",
    "# The idea is that filter_offset_type() takes in two arguments: the offset in terms of coordinates, a relative heading \n",
    "# as well as the type of object to be offset from. In this case, we want it to be somehwere between -10 and 10 units\n",
    "# offset relative to a camera's x position, somehwere between 20 and 40 units offset relative to some camera's y position, \n",
    "# and we dont care about the offset relative to the camera's z position. We also dont care about the relative heading difference.\n",
    "\n",
    "filtered_ids = filtered_world.get_traj_key()\n",
    "print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Total execution time is: %s seconds\" % (time.time() - start_time))\n",
    "print(\"Device Details: \\n Processor: AMD Ryzen 7 5800H \\n RAM Size: 16GB \\n Graphics Card: NVIDIA GeForce RTX 3060 Laptop\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "# traffic-scene-shorter (length of 4 seconds): runtime of 81.82859063148499 seconds\n",
    "# traffic-scene (length of 20 seconds): runtime of 98.58345794677734 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 4)\n",
      "(88, 4)\n",
      "incorrect length: 88\n",
      "(24, 4)\n",
      "incorrect length: 24\n",
      "output video files ./output/trafficScene_cam1car-4-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4,./output/trafficScene_cam1car-6-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4,./output/trafficScene_cam1car-9-a51f7975-379b-4df4-b4ce-7ba216f0b05e.mp4\n"
     ]
    }
   ],
   "source": [
    "filtered_world.get_video([cam_id], boxed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\").interval(0, fps * 3)\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "# # render tracking video\n",
    "# filtered_world.get_video([cam_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40,\n",
    "#                                                         type=\"camera\")\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=None, type=\"camera\")\n",
    "\n",
    "# filtered_world = filtered_world.filter_heading(-5, 5)\n",
    "# # Filters for objects that have heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40 \\\n",
    "#                                                                                   and -5 <= (camera.heading - obj.heading) <= 5,\n",
    "#                                                         type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=(-5, 5), type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car left of ego by 0.25 \n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def left_of(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y)\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=left_of, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # badAngle = Range(10, 20) deg\n",
    "# # Car left of ego by 0.25,\n",
    "# # \tfacing badAngle relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def filter(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y) and 10 <= (camera.heading - obj.heading) <= 20\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=filter, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roadDirection(x, y, z):\n",
    "#     # TODO: Implement\n",
    "#     # Returns the direction (in 360 degree angle form) of the road at the coordinates (x, y, z)\n",
    "#     # If their is no such road, returns a value of None\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FURTHER QUERIES WILL USE THE OPTION 1 LISTED ABOVE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# weather = Uniform(\"sunny\", \"rainy\", \"thunder\")\n",
    "# time = Range(10, 12)\n",
    "#\n",
    "# ego = car on road\n",
    "# otherCar = Car ahead of ego by Range(4, 19)\n",
    "# require not (otherCar in intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# spot = OrientedPoint on curb\n",
    "# ego = Car at (spot offset by (Range(2,4), Range(5,10)))\n",
    "# sideCar = Car left of spot by Range(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(car, numCars):\n",
    "#     for i in range(numCars):\n",
    "#         car = Car ahead of car by Range(4, 5)\n",
    "#         leftCar = Car left of car by Normal(2, 0.1), facing roadDirection\n",
    "#         rightCar = Car right of car by Normal(3, 0.1), facing Range(0, 10) deg relative to ego.heading\n",
    "#     return leftCar, rightCar\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point, with visible_distance 200\n",
    "\n",
    "# leftCar, rightCar = placeObjs(ego, 2)\n",
    "# require (distance to leftCar) < 200\n",
    "# require (distance to rightCar) < 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(numPeds):\n",
    "#     for i in range(numPeds):\n",
    "#         Pedestrian offset by Range(-5, 5) @ Range(0, 200),\n",
    "#             facing Range(-120, 120) deg relative to ego.heading\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point,\n",
    "#         with visibleDistance 200\n",
    "\n",
    "# placeObjs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50, \n",
    "#         with viewAngle 135 deg\n",
    "# ped = Pedestrian on roadsOrIntersections,\n",
    "#         with regionContainedIn roadRegion,\n",
    "#         facing Range(-180, 180) deg\n",
    "\n",
    "# require abs(relative heading of ped from ego) > 70 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# offset = Uniform(-1, 1) * Range(90, 180) deg\n",
    "\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing offset relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# otherCar = Car on visible road,\n",
    "#             facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# require (distance from ego to otherCar) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# other1 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# other2 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# require abs(relative heading of other1 from other2) > 100 deg\n",
    "# require (distance from ego to intersectionRegion) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# point1 = OrientedPoint ahead of ego by Range(0, 40)\n",
    "# Car at (point1 offset by Range(-1, 1) & 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# oppositeCar = Car offset by (Range(-10, -1), Range(0, 50)),\n",
    "#     facing Range(140, 180) deg relative to ego.heading\n",
    "\n",
    "# point2 = OrientedPoint ahead of oppositeCar by Range(0, 40)\n",
    "# Car at (point2 offset by Range(-1, 1) @ 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# lanesWithRightLane = filter(lambda i: i._laneToRight, network.laneSections)\n",
    "# egoLane = Uniform(*lanesWithRightLane)\n",
    "\n",
    "# ego = Car on egoLane,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection\n",
    "# cutInCar = Car offset by Range(0, 4) @ Range(0, 5),\n",
    "#             facing -1*Range(15, 30) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think there are 3 main things that need to now be implemented in Apperception to allow incorparation with Scenic:\n",
    "# 1. A way to have the arbitrary filters that were possible in the old API (the predicate lambda functions that could be passed)\n",
    "# 2. A way to have filters with regardes to other objects. For example, I could say I want \"cars that are to the left of a bus by 0.25m\" or such. I would assume this could also be implemented as a lambda function filter (I have included an example fo this in the scenic_equivelants notebook).\n",
    "# 3. Some way to not only recognize what the type of an object is, but recognize the type of point it is on. For example, recognizing that the Car is on a road, or that the Car is in an intersection (this is something that is done quite a lot in Scenic).\n",
    "#      - For this, we might not have to incorporate it into apperception, and can make it the users responsibility (and they can create their own filters that do this), but I am not too sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b9f45d2c0c5940d48526f9dac9a46c8afda5d718c8f108cd3f22cd85be16c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
