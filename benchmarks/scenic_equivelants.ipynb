{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the .apperception_cache if it exists, as to avoid DB conflict errors\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\"apperception\"))\n",
    "\n",
    "dirpath = os.path.join('.apperception_cache')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "\n",
    "dirpath = os.path.join('output')\n",
    "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "    shutil.rmtree(dirpath)\n",
    "os.mkdir(dirpath)\n",
    "\n",
    "# This piece of code is unsafe, and should not be run if not needed. \n",
    "# It serves for test purposes when one recieves a \"dead kernel\" error.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AMBER DATA #####\n",
    "# import sys\n",
    "# sys.path.append(os.path.join(os.getcwd(),\"apperception\"))\n",
    "\n",
    "# ### IMPORTS\n",
    "# import lens\n",
    "# import point\n",
    "# from new_world import empty_world\n",
    "\n",
    "# # Let's define some attribute for constructing the world first\n",
    "# name = \"trafficScene\"  # world name\n",
    "# units = \"metrics\"  # world units\n",
    "# video_file = \"amber_videos/traffic-scene.mp4\"  # example video file\n",
    "# lens_attrs = {\"fov\": 120, \"cam_origin\": (0, 0, 0), \"skew_factor\": 0}\n",
    "# point_attrs = {\"p_id\": \"p1\", \"cam_id\": \"cam1\", \"x\": 0, \"y\": 0, \"z\": 0, \"time\": None, \"type\": \"pos\"}\n",
    "# camera_attrs = {\"ratio\": 0.5}\n",
    "# fps = 30\n",
    "\n",
    "# # 1. define a world\n",
    "# traffic_world = empty_world(name)\n",
    "\n",
    "# # 2. construct a camera\n",
    "# fov, res, cam_origin, skew_factor = (\n",
    "#     lens_attrs[\"fov\"],\n",
    "#     [1280, 720],\n",
    "#     lens_attrs[\"cam_origin\"],\n",
    "#     lens_attrs[\"skew_factor\"],\n",
    "# )\n",
    "# cam_lens = lens.PinholeLens(res, cam_origin, fov, skew_factor)\n",
    "\n",
    "# pt_id, cam_id, x, y, z, time, pt_type = (\n",
    "#     point_attrs[\"p_id\"],\n",
    "#     point_attrs[\"cam_id\"],\n",
    "#     point_attrs[\"x\"],\n",
    "#     point_attrs[\"y\"],\n",
    "#     point_attrs[\"z\"],\n",
    "#     point_attrs[\"time\"],\n",
    "#     point_attrs[\"type\"],\n",
    "# )\n",
    "# location = point.Point(pt_id, cam_id, (x, y, z), time, pt_type)\n",
    "\n",
    "# ratio = camera_attrs[\"ratio\"]\n",
    "\n",
    "# # ingest the camera into the world\n",
    "# traffic_world = traffic_world.add_camera(\n",
    "#     cam_id=cam_id,\n",
    "#     location=location,\n",
    "#     ratio=ratio,\n",
    "#     video_file=video_file,\n",
    "#     metadata_identifier=name + \"_\" + cam_id,\n",
    "#     lens=cam_lens,\n",
    "# )\n",
    "\n",
    "# # Call execute on the world to run the detection algorithm and save the real data to the database\n",
    "# recognized_world = traffic_world.recognize(cam_id)\n",
    "\n",
    "# volume = traffic_world.select_intersection_of_interest_or_use_default(cam_id=cam_id)\n",
    "\n",
    "# cams = traffic_world.get_camera()\n",
    "# lens = traffic_world.get_len()\n",
    "# # ids = traffic_world.get_id()\n",
    "# print(\"cameras are\", cams)\n",
    "# print(\"lens are\", lens)\n",
    "# # print(\"ids are\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NUSCENES DATA #####\n",
    "from new_world import empty_world, World\n",
    "from camera import Camera\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "World.db.reset()\n",
    "\n",
    "name = 'ScenicWorld' # world name\n",
    "units = 'metrics'      # world units\n",
    "user_data_dir = os.path.join(\"v1.0-mini\")\n",
    "\n",
    "with open('df_sample_data.pickle', \"rb\") as f:\n",
    "    df_sample_data = pickle.loads(f.read())\n",
    "with open('df_annotation.pickle', \"rb\") as f:\n",
    "    df_annotation = pickle.loads(f.read())\n",
    "\n",
    "world = empty_world(name=name)\n",
    "\n",
    "from camera_config import fetch_camera_config\n",
    "# scenes = [\"scene-0061\", \"scene-0103\",\"scene-0553\", \"scene-0655\", \"scene-0757\", \"scene-0796\", \"scene-0916\", \"scene-1077\", \"scene-1094\", \"scene-1100\"]\n",
    "scenes = [\"scene-0061\"]\n",
    "for scene in scenes:\n",
    "    config = fetch_camera_config(scene, df_sample_data)\n",
    "    camera = Camera(config=config, id=scene)\n",
    "    world = world.add_camera(camera)\n",
    "    df_config = df_sample_data[df_sample_data['scene_name'] == scene][['sample_token']]\n",
    "    df_ann = df_annotation.join(df_config.set_index('sample_token'), on='sample_token', how='inner')\n",
    "    world = world.recognize(camera, df_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = world.get_camera()\n",
    "# ids = world.get_id()\n",
    "print(\"cameras are\", cams[:2])\n",
    "# print(\"ids are\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "### Scenic Code ###\n",
    "# ego = Car\n",
    "# Car offset by (Range(-10, 10), Range(20, 40))\n",
    "\n",
    "### Apperception Query ###\n",
    "filtered_world = world.filter_traj_type(\"vehicle.car\")\n",
    "\n",
    "## OPTION 1 ###\n",
    "filtered_world = filtered_world.filter_relative_to_type(x_range=(-1000, 1000), y_range=(-1000, 5000), z_range=(-1000, 1000),\n",
    "                                                        type=\"camera\")\n",
    "# The idea is that the user passes in a lambda function, that specifies the relationship that must be met between the queried\n",
    "# object, and some object of the type passed to the function. In this case, the lambda function filters such that the offset \n",
    "# is between -10 and 10 in the x direction, and between 20 and 40 in the y direction, relative to some camera.\n",
    "\n",
    "### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_pred_relative_to_type(pred = lambda obj: (cam.x - 10) <= obj.x <= (cam.x + 10) and (cam.y - 15) <= obj.y <= (cam.y + 70))\n",
    "# The idea is that filter_offset_type() takes in two arguments: the offset in terms of coordinates, a relative heading \n",
    "# as well as the type of object to be offset from. In this case, we want it to be somehwere between -10 and 10 units\n",
    "# offset relative to a camera's x position, somehwere between 20 and 40 units offset relative to some camera's y position, \n",
    "# and we dont care about the offset relative to the camera's z position. We also dont care about the relative heading difference.\n",
    "\n",
    "filtered_ids = filtered_world.get_traj_key()\n",
    "print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Total execution time is: %s seconds\" % (time.time() - start_time))\n",
    "# print(\"Device Details: \\n Processor: AMD Ryzen 7 5800H \\n RAM Size: 16GB \\n Graphics Card: NVIDIA GeForce RTX 3060 Laptop\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "# traffic-scene-shorter (length of 4 seconds): runtime of 81.82859063148499 seconds\n",
    "# traffic-scene (length of 20 seconds): runtime of 98.58345794677734 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_world.get_traj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get results if using Amber Data ###\n",
    "# filtered_world.get_video([cam_id], boxed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\").interval(0, fps * 3)\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)\n",
    "\n",
    "# # render tracking video\n",
    "# filtered_world.get_video([cam_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40,\n",
    "#                                                         type=\"camera\")\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=None, type=\"camera\")\n",
    "\n",
    "# filtered_world = filtered_world.filter_heading(-5, 5)\n",
    "# # Filters for objects that have heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car offset by (Range(-10, 10), Range(20, 40)), \n",
    "# # \tfacing Range(-5, 5) deg relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=lambda obj, camera: -10 <= (camera.x - obj.x) <= 10 \\\n",
    "#                                                                                   and 20 <= (camera.y - obj.y) <= 40 \\\n",
    "#                                                                                   and -5 <= (camera.heading - obj.heading) <= 5,\n",
    "#                                                         type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# ### OPTION 2 ###\n",
    "# filtered_world = filtered_world.filter_relative_to_type(offset=((-10, 10), (20, 40), None), heading=(-5, 5), type=\"camera\")\n",
    "# # Now filtering for a relative heading between -5 and 5 degrees\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # Car left of ego by 0.25 \n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def left_of(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y)\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=left_of, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Scenic Code ###\n",
    "# # ego = Car\n",
    "# # badAngle = Range(10, 20) deg\n",
    "# # Car left of ego by 0.25,\n",
    "# # \tfacing badAngle relative to ego\n",
    "\n",
    "# ### Apperception Query ###\n",
    "# filtered_world = recognized_world.filter_traj_type(\"car\")\n",
    "\n",
    "# ## OPTION 1 ###\n",
    "# def filter(obj, camera):\n",
    "#     expec_x = obj.x + 0.25 * np.cos(camera.heading)\n",
    "#     expec_y = obj.y - 0.25 * np.sin(camera.heading)\n",
    "#     # Should also allow some sort of variation, to account for noise (and since exact equality is unlikley)\n",
    "#     return (expec_x == camera.x) and (expec_y == camera.y) and 10 <= (camera.heading - obj.heading) <= 20\n",
    "\n",
    "# filtered_world = filtered_world.filter_relative_to_type(relative=filter, type=\"camera\")\n",
    "# # Now filtering such that the car is left of ego by 0.25 units\n",
    "\n",
    "# ### OPTION 2 ##\n",
    "# # Not possible\n",
    "\n",
    "# filtered_ids = filtered_world.get_traj_key()\n",
    "# print(\"filtered_ids are\", filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roadDirection(x, y, z):\n",
    "#     # TODO: Implement\n",
    "#     # Returns the direction (in 360 degree angle form) of the road at the coordinates (x, y, z)\n",
    "#     # If their is no such road, returns a value of None\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FURTHER QUERIES WILL USE THE OPTION 1 LISTED ABOVE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# weather = Uniform(\"sunny\", \"rainy\", \"thunder\")\n",
    "# time = Range(10, 12)\n",
    "#\n",
    "# ego = car on road\n",
    "# otherCar = Car ahead of ego by Range(4, 19)\n",
    "# require not (otherCar in intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# spot = OrientedPoint on curb\n",
    "# ego = Car at (spot offset by (Range(2,4), Range(5,10)))\n",
    "# sideCar = Car left of spot by Range(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(car, numCars):\n",
    "#     for i in range(numCars):\n",
    "#         car = Car ahead of car by Range(4, 5)\n",
    "#         leftCar = Car left of car by Normal(2, 0.1), facing roadDirection\n",
    "#         rightCar = Car right of car by Normal(3, 0.1), facing Range(0, 10) deg relative to ego.heading\n",
    "#     return leftCar, rightCar\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point, with visible_distance 200\n",
    "\n",
    "# leftCar, rightCar = placeObjs(ego, 2)\n",
    "# require (distance to leftCar) < 200\n",
    "# require (distance to rightCar) < 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# def placeObjs(numPeds):\n",
    "#     for i in range(numPeds):\n",
    "#         Pedestrian offset by Range(-5, 5) @ Range(0, 200),\n",
    "#             facing Range(-120, 120) deg relative to ego.heading\n",
    "\n",
    "# spawn_point = 207.26 @ 8.72\n",
    "# ego = Car at spawn_point,\n",
    "#         with visibleDistance 200\n",
    "\n",
    "# placeObjs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50, \n",
    "#         with viewAngle 135 deg\n",
    "# ped = Pedestrian on roadsOrIntersections,\n",
    "#         with regionContainedIn roadRegion,\n",
    "#         facing Range(-180, 180) deg\n",
    "\n",
    "# require abs(relative heading of ped from ego) > 70 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# offset = Uniform(-1, 1) * Range(90, 180) deg\n",
    "\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing offset relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# otherCar = Car on visible road,\n",
    "#             facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# require (distance from ego to otherCar) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# other1 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# other2 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# require abs(relative heading of other1 from other2) > 100 deg\n",
    "# require (distance from ego to intersectionRegion) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# point1 = OrientedPoint ahead of ego by Range(0, 40)\n",
    "# Car at (point1 offset by Range(-1, 1) & 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection\n",
    "\n",
    "# oppositeCar = Car offset by (Range(-10, -1), Range(0, 50)),\n",
    "#     facing Range(140, 180) deg relative to ego.heading\n",
    "\n",
    "# point2 = OrientedPoint ahead of oppositeCar by Range(0, 40)\n",
    "# Car at (point2 offset by Range(-1, 1) @ 0),\n",
    "#     facing Range(-15, 15) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenic Code ###\n",
    "# lanesWithRightLane = filter(lambda i: i._laneToRight, network.laneSections)\n",
    "# egoLane = Uniform(*lanesWithRightLane)\n",
    "\n",
    "# ego = Car on egoLane,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection\n",
    "# cutInCar = Car offset by Range(0, 4) @ Range(0, 5),\n",
    "#             facing -1*Range(15, 30) deg relative to roadDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think there are 3 main things that need to now be implemented in Apperception to allow incorparation with Scenic:\n",
    "# 1. A way to have the arbitrary filters that were possible in the old API (the predicate lambda functions that could be passed)\n",
    "# 2. A way to have filters with regardes to other objects. For example, I could say I want \"cars that are to the left of a bus by 0.25m\" or such. I would assume this could also be implemented as a lambda function filter (I have included an example fo this in the scenic_equivelants notebook).\n",
    "# 3. Some way to not only recognize what the type of an object is, but recognize the type of point it is on. For example, recognizing that the Car is on a road, or that the Car is in an intersection (this is something that is done quite a lot in Scenic).\n",
    "#      - For this, we might not have to incorporate it into apperception, and can make it the users responsibility (and they can create their own filters that do this), but I am not too sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36b9f45d2c0c5940d48526f9dac9a46c8afda5d718c8f108cd3f22cd85be16c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
