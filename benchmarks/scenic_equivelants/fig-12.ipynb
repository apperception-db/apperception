{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e564f2b-e214-4d07-bade-bf55c8190b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "# %pip install .\n",
    "import time\n",
    "from os import environ\n",
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50, \n",
    "#         with viewAngle 135 deg\n",
    "# ped = Pedestrian on roadsOrIntersections,\n",
    "#         with regionContainedIn roadRegion,\n",
    "#         facing Range(-180, 180) deg\n",
    "\n",
    "# require abs(relative heading of ped from ego) > 70 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9d504-225b-4e92-bd7d-c49e70caa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "if environ[\"AP_PORT\"] is None:\n",
    "    environ[\"AP_PORT\"] = str(input('port'))\n",
    "# README command uses port=25432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbe6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with (\n",
    "  select *\n",
    "  from item_traj\n",
    "  where item_traj.object_type = 'pedestrian'\n",
    ") as pedestrians\n",
    "\n",
    "select *\n",
    "from pedestrians as t1\n",
    "join Cameras as cam on t1.cameraId = Cameras.id\n",
    "where\n",
    "  t1.heading < 180 and t1.heading > -180 and\n",
    "  (contained(t1.traj, road_segment(\"roads\"), cam.timestamp) or\n",
    "   contained(t1.traj, road_segment(\"Intersections\"), cam.timestamp)) and\n",
    "  (facingRelative(t1.heading, cam.egoHeading, cam.timestamp) < -70 OR\n",
    "  facingRelative(t1.heading, cam.egoHeading, cam.timestamp) > 70) AND\n",
    "  facingRelative(cam.egoHeading, road_direction(cam.ego_translation, Cameras.egoHeading), cam.timestamp) >= -15 AND\n",
    "  facingRelative(cam.egoHeading, road_direction(cam.ego_translation, Cameras.egoHeading), cam.timestamp) <= 15 AND\n",
    "  DISTANCE(cam.egoTranslation, t1.centroid, cam.timestamp) < 50 AND\n",
    "  viewAngle(t1.traj, cam.egoHeading, cam.ego_translation, cam.timestamp) < 135\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b38598",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Prepare the world for queries ###\n",
    "from apperception.world import empty_world\n",
    "# directly ingest the mini dataset and boston roadnetwork if needed\n",
    "# from apperception.database import database\n",
    "# database.reset()\n",
    "# from benchmarks.ingest_scenic_data import ingest_data\n",
    "# ingest_data()\n",
    "\n",
    "name = 'ScenicWorld' # world name\n",
    "world = empty_world(name=name)\n",
    "\n",
    "### Query ###\n",
    "from apperception.utils import F\n",
    "# world = world.filter(lambda obj: obj.object_type == 'human.pedestrian.adult' or\n",
    "#                                  obj.object_type == 'human.pedestrian.wheelchair' or\n",
    "#                                  obj.object_type == 'human.pedestrian.personal_mobility' or\n",
    "#                                  obj.object_type == 'human.pedestrian.construction_worker' or\n",
    "#                                  obj.object_type == 'human.pedestrian.police_officer' or\n",
    "#                                  obj.object_type == 'human.pedestrian.child' or\n",
    "#                                  obj.object_type == 'human.pedestrian.stroller')\n",
    "def pred(obj1, cam):\n",
    "    return (\n",
    "        obj1.heading < 180 and obj1.heading > -180 and\n",
    "        F.contained(cam.ego, F.road_segment(\"road\"), cam.timestamp) and\n",
    "        (F.contained(obj1.traj, F.road_segment(\"road\"), cam.timestamp) or\n",
    "            F.contained(obj1.traj, F.road_segment(\"intersection\"))) and\n",
    "        (F.facing_relative(obj1, cam.ego, cam.timestamp) < -70 or\n",
    "            F.facing_relative(obj1, cam.ego, cam.timestamp) >= 70) and\n",
    "        F.facing_relative(cam.ego, F.road_direction(cam.ego, cam.timestamp), cam.timestamp) >= -15 and\n",
    "        F.facing_relative(cam.ego, F.road_direction(cam.ego, cam.timestamp), cam.timestamp) <= 15 and\n",
    "        F.distance(cam.ego, obj1, cam.timestamp) < 50 and\n",
    "        F.viewAngle(obj1, cam.ego, cam.timestamp) < 135\n",
    "    )\n",
    "\n",
    "# With road direction\n",
    "world = world.filter(\"lambda obj1, cam: \" +\n",
    "        \"F.like(obj1.object_type, 'human.pedestrian%') and \" +\n",
    "        \"F.contained(cam.ego, F.road_segment('road')) and \" +\n",
    "        \"(F.contained(obj1.traj, F.road_segment('lane'), cam.timestamp) or \" + \n",
    "            \"F.contained(obj1.traj, F.road_segment('intersection'), cam.timestamp)) and \" +\n",
    "        \"(F.facing_relative(obj1, cam.ego, cam.timestamp) < -70 or \" + \n",
    "            \"F.facing_relative(obj1, cam.ego, cam.timestamp) >= 70) and \" + \n",
    "        \"F.facing_relative(cam.ego, F.road_direction(cam.ego, cam.timestamp, cam.ego), cam.timestamp) >= -15 and \" +\n",
    "        \"F.facing_relative(cam.ego, F.road_direction(cam.ego, cam.timestamp, cam.ego), cam.timestamp) <= 15 and \" +\n",
    "        \"F.distance(cam.ego, obj1, cam.timestamp) < 50 and \" +\n",
    "        \"F.view_angle(obj1, cam.ego, cam.timestamp) < 67.5\")\n",
    "\n",
    "start = time.time()\n",
    "# keys = world.get_traj_key()\n",
    "id_time_camId_filename = world.get_id_time_camId_filename(num_joined_tables=1)\n",
    "\n",
    "end = time.time()\n",
    "print(format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultImages = dict() # maping from image -> (itemId, timestamp, camId) that it was found at\n",
    "for result in id_time_camId_filename:\n",
    "    itemId, timestamp, camId, filename = result\n",
    "    filename = filename.split(\"/\")[-1] # use split so that prefix path is not included in filename\n",
    "    resultImages[filename] = (itemId, timestamp, camId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a64ad-189b-40cd-8e65-fd973b05a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "\n",
    "# experiment_data_dir =  \"data/nuscenes/experiment_data\"\n",
    "experiment_data_dir = '/work/apperception/data/nuScenes/full-dataset-v1.0/Trainval/experiment_data'\n",
    "def show_images(images, sample=None):\n",
    "    if sample is not None:\n",
    "        images = [i for i in images]\n",
    "        random.shuffle(images)\n",
    "        images = images[:sample]\n",
    "    \n",
    "    plt.figure(figsize=(60,30))\n",
    "    columns = 3\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        print(\"image\", image)\n",
    "        img = mpimg.imread(os.path.join(experiment_data_dir, image))\n",
    "        print(\"loaded\")\n",
    "        plt.subplot(len(images) // columns + 1, columns, i + 1)\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6eae8c-f909-4a93-a25d-c875fec61b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(list(resultImages.keys()), sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_imgs = ['n008-2018-05-21-11-06-59-0400__CAM_FRONT__1526915471412465.jpg', 'n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707917112404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385153162404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385153662404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385154162404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385154662404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385155162404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385158662404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385159162404.jpg', 'n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385159662404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656805162404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656805662415.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656806162404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656806612404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656807162404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656807662404.jpg', 'n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535656808162404.jpg', 'n008-2018-08-30-15-52-26-0400__CAM_FRONT__1535659404362404.jpg', 'n008-2018-08-30-15-52-26-0400__CAM_FRONT__1535659404762404.jpg', 'n008-2018-08-30-15-52-26-0400__CAM_FRONT__1535659405262404.jpg', 'n008-2018-08-30-15-52-26-0400__CAM_FRONT__1535659405762404.jpg', 'n008-2018-08-30-15-52-26-0400__CAM_FRONT__1535659406262404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535728830362404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729326412404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729326912404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729327412404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729327912404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729328412404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729328912404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729329412404.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729329912795.jpg', 'n008-2018-08-31-11-19-57-0400__CAM_FRONT__1535729330362404.jpg', 'n008-2018-08-31-11-37-23-0400__CAM_FRONT__1535730293412404.jpg', 'n008-2018-08-31-11-56-46-0400__CAM_FRONT__1535731236162404.jpg', 'n008-2018-08-31-11-56-46-0400__CAM_FRONT__1535731236662404.jpg', 'n008-2018-08-31-11-56-46-0400__CAM_FRONT__1535731237112404.jpg', 'n008-2018-08-31-11-56-46-0400__CAM_FRONT__1535731237612404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293291162404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293291662404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293292162404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293292662404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293293162404.jpg', 'n008-2018-09-18-13-41-50-0400__CAM_FRONT__1537293293662404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299143862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299144362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299144862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299145362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299145862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299146362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299146862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299147362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299147862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299148362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299148862404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299149412404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299229362404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299239112404.jpg', 'n008-2018-09-18-15-26-58-0400__CAM_FRONT__1537299240112404.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b0d99-492d-4015-b2ad-0b04c37b30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(expected_imgs, sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = 0\n",
    "missing_images = []\n",
    "for expected_img in expected_imgs:\n",
    "    if expected_img not in resultImages.keys():\n",
    "        missing += 1\n",
    "        missing_images.append(expected_img)\n",
    "print(\"Percentage of expected results missing from query: \", missing, \"/\", len(expected_imgs), \"=\", 100 * missing / len(expected_imgs), \"%\")\n",
    "show_images(missing_images, sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104623e9-7c09-4ff1-945c-53b11c7b8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = 0\n",
    "extra_images = {}\n",
    "for img in resultImages.keys():\n",
    "    if img not in expected_imgs:\n",
    "        extra += 1\n",
    "        extra_images[img] = resultImages[img]\n",
    "print(\"Percentage of images in query but not in expected results: \", extra, \"/\", len(resultImages.keys()), \"=\", 100 * extra / len(resultImages.keys()), \"%\")\n",
    "show_images(extra_images.keys(), sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Getting info about tthe extra images ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "for img in extra_images:\n",
    "    itemId, timestamp, camId = extra_images[img]\n",
    "    prefix = \"samples/CAM_FRONT/\"\n",
    "    query = f\"SELECT \\'Cameras: \\', Cameras.filename, roadDirection(Cameras.egoTranslation, Cameras.timestamp, Cameras.egoHeading), Cameras.egoHeading, ST_X(Cameras.egoTranslation), ST_Y(Cameras.egoTranslation), ST_Z(Cameras.egoTranslation), \\'Pedestrian: \\', getX(table_0.trajCentroids, Cameras.timestamp), getY(table_0.trajCentroids, Cameras.timestamp), ST_Z(valueAtTimestamp(table_0.trajCentroids, Cameras.timestamp)), valueAtTimestamp(table_0.itemHeadings, Cameras.timestamp) FROM Item_General_Trajectory AS table_0, Cameras\" + \\\n",
    "                f\" WHERE Cameras.filename = \\'{prefix + img}\\' AND table_0.itemId = \\'{itemId}\\'\"\n",
    "    database.cursor.execute(query)\n",
    "    result = database.cursor.fetchall()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb14000",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Seeing what predicates are not being satisfied for missing images ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffaaec-1cbe-48c3-8aa0-80982756c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume for now that there is no problem with the commented out predicates, since they are relativley simple\n",
    "predicates = [\n",
    "    # \"(table_0.objectType LIKE 'human.pedestrian%')\",\n",
    "    \"contained(Cameras.egoTranslation, roadSegment('road'))\",\n",
    "    \"(contained(table_0.trajCentroids, roadSegment('road'), Cameras.timestamp) OR contained(table_0.trajCentroids, roadSegment('intersection'), Cameras.timestamp))\",\n",
    "    \"((facingRelative(table_0.itemHeadings, Cameras.egoHeading, Cameras.timestamp)<(-70)) OR (facingRelative(table_0.itemHeadings, Cameras.egoHeading, Cameras.timestamp)>=70))\",\n",
    "    \"(facingRelative(Cameras.egoHeading, roadDirection(Cameras.egoTranslation, Cameras.timestamp, Cameras.egoHeading), Cameras.timestamp)>=(-15))\",\n",
    "    \"(facingRelative(Cameras.egoHeading, roadDirection(Cameras.egoTranslation, Cameras.timestamp, Cameras.egoHeading), Cameras.timestamp)<=15)\",\n",
    "    # \"(distance(Cameras.egoTranslation, table_0.trajCentroids, Cameras.timestamp)<50)\",\n",
    "    # \"(viewAngle(table_0.trajCentroids, Cameras.egoHeading, Cameras.egoTranslation, Cameras.timestamp)<135)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85118369-323c-4ff2-9c32-aa43093a0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "preds_missed = {}\n",
    "for img in missing_images:    \n",
    "    prefix = \"samples/CAM_FRONT/\"\n",
    "    query = f\"SELECT COUNT(*) FROM Item_General_Trajectory AS table_0, Cameras\" + \\\n",
    "                f\" WHERE Cameras.filename = \\'{prefix + img}\\' AND (table_0.objectType LIKE 'human.pedestrian%') AND (distance(Cameras.egoTranslation, table_0.trajCentroids, Cameras.timestamp)<50) AND (viewAngle(table_0.trajCentroids, Cameras.egoHeading, Cameras.egoTranslation, Cameras.timestamp)<135)\" \n",
    "    database.cursor.execute(query)\n",
    "    result = database.cursor.fetchall()\n",
    "    num_peds = result[0][0]\n",
    "    # For now we only take a look at images with one pedestrian (so that we know that that one pedestrian is the one that is supposed to satisfy the condition)\n",
    "    # (if still experiencing problems after fixing the errors with the images with only one pedestrian, we can expand)\n",
    "    if num_peds != -1:\n",
    "        # print(\"------\", img, \"------\")\n",
    "        for predicate in predicates:\n",
    "            prefix = \"samples/CAM_FRONT/\"\n",
    "            query = f\"SELECT true FROM Item_General_Trajectory AS table_0, Cameras\" + \\\n",
    "                        f\" WHERE Cameras.filename = \\'{prefix + img}\\' AND (table_0.objectType LIKE 'human.pedestrian%') AND (distance(Cameras.egoTranslation, table_0.trajCentroids, Cameras.timestamp)<50) AND (viewAngle(table_0.trajCentroids, Cameras.egoHeading, Cameras.egoTranslation, Cameras.timestamp)<135) AND \" + \\\n",
    "                        predicate \n",
    "            database.cursor.execute(query)\n",
    "            result = database.cursor.fetchall()\n",
    "            print(result)\n",
    "            # if the predictae was not satisfied\n",
    "            if len(result) == 0: \n",
    "                if predicate not in preds_missed:\n",
    "                    preds_missed[predicate] = 0\n",
    "                preds_missed[predicate] += 1\n",
    "                print(predicate)\n",
    "                query = f\"SELECT {predicate}, facingRelative(Cameras.egoHeading, roadDirection(Cameras.egoTranslation, Cameras.timestamp, Cameras.egoHeading), Cameras.timestamp), roadDirection(Cameras.egoTranslation, Cameras.timestamp, Cameras.egoHeading), ST_X(Cameras.egoTranslation), ST_Y(Cameras.egoTranslation), ST_Z(Cameras.egoTranslation), Cameras.egoHeading FROM Item_General_Trajectory AS table_0, Cameras\" + \\\n",
    "                        f\" WHERE Cameras.filename = \\'{prefix + img}\\' AND (table_0.objectType LIKE 'human.pedestrian%') AND (distance(Cameras.egoTranslation, table_0.trajCentroids, Cameras.timestamp)<50) AND (viewAngle(table_0.trajCentroids, Cameras.egoHeading, Cameras.egoTranslation, Cameras.timestamp)<135)\" \n",
    "                database.cursor.execute(query)\n",
    "                result = database.cursor.fetchall()\n",
    "                print(result)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in preds_missed:\n",
    "    print(pred, preds_missed[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e51a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c9f2372a2bfaf539cf701a38e7f23ab828911ee177c2e7bc9c32aa1f4b546df"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
