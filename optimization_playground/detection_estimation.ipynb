{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4ea91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from segment_mapping import *\n",
    "import matplotlib.pyplot as plt\n",
    "from apperception.utils import fetch_camera_config, fetch_camera_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4f95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_config = fetch_camera_config('samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385108912404.jpg', database)\n",
    "# test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f3e155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mapping = map_imgsegment_roadsegment(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a081db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file_path = '/home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385108912404.jpg'\n",
    "# visualization(test_file_path, test_config, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8abf4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "strong_sort/deep/reid/torchreid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n",
      "/home/yongming/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "YOLOv5 ðŸš€ 2022-8-20 Python-3.8.3 torch-1.12.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "%cd ../Yolov5_StrongSORT_OSNet\n",
    "import sample_frame_tracker\n",
    "# full_img_detection = sample_frame_tracker.run(test_file_path, save_vid=True, detect_only=True)\n",
    "# full_img_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96223b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_detection(test_file_path, full_img_detection):\n",
    "    import cv2\n",
    "    test_frame = cv2.imread(test_file_path)\n",
    "    for obj_idx, detection in full_img_detection.items():\n",
    "        obj_cls, bbox = detection\n",
    "        if obj_cls == 'car':\n",
    "            x,y,w,h = list(map(int,bbox))\n",
    "            cv2.rectangle(test_frame,(x-w//2,y-h//2),(x+w//2,y+h//2),(0,255,0),2)\n",
    "            cv2.putText(test_frame, '_'.join([obj_cls, str(obj_idx)]), (x+w//2+5,y+h//2+5),0,0.3,(0,255,0))\n",
    "    cv2.imshow('detection', test_frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e94f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_estimation.utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e8f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ego car trajectory\n",
    "def prepare_ego(test_video):\n",
    "    ego_trajectory = get_ego_trajectory(test_video)\n",
    "    video_trajectory = fetch_camera_trajectory(test_video, database)\n",
    "    sorted_ego_configs = [fetch_camera_config(e['fileName'], database) for e in video_trajectory]\n",
    "    return sorted_ego_configs, ego_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b0672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ego_speed = get_ego_speed(ego_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f57a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [time_to_nearest_frame(test_video, point.timestamp) for point in ego_trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6659ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# test_timestamp = datetime.datetime(2018, 8, 27, 8, 51, 32, 162404, tzinfo=datetime.timezone.utc)\n",
    "# timestamp_to_nearest_trajectory(ego_trajectory, test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764ed7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_point = [1772, 865, 0.0]\n",
    "# point_to_nearest_trajectory(test_point, ego_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0fa4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_estimation.sample_plan_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73351a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/yongming/workspace/research/apperception_new_local/boston-seaport'\n",
    "test_img_base_dir = '/home/yongming/workspace/research/apperception/v1.0-mini/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af8f35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(os.path.join(base_dir, f'frames.pickle'), \"rb\") as f:\n",
    "#     df_sample_data = pickle.loads(f.read())\n",
    "# df_sample_data\n",
    "# i = 0\n",
    "# for frame in df_sample_data['scene-0655-CAM_FRONT']['frames']:\n",
    "#     if frame[2] == 1194:\n",
    "#         print(i)\n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848684da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_estimation.detection_estimation import *\n",
    "# ### Integration\n",
    "# target_config_idx = 218\n",
    "# video = 'scene-0655-CAM_FRONT'\n",
    "# configs = df_sample_data[video]\n",
    "# sorted_ego_config = [dict(zip(configs['columns'], frame))\n",
    "#                      for frame in configs['frames']]\n",
    "# len(sorted_ego_config)\n",
    "# # all_car_loc3d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3071563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_ego_config = sorted_ego_config[target_config_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e88fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_loc3d_ground_truth = [(1991, 874), (1949.181, 873.164)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8942c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "def generate_sample_plan_once(video, ego_config, mapping, next_frame_num, car_loc3d=None, target_car_detection=None, all_detection_info=None):\n",
    "    if all_detection_info is None:\n",
    "        assert target_car_detection and car_loc3d\n",
    "        x,y,w,h = list(map(int, target_car_detection))\n",
    "        car_loc2d = (x, y+h//2)\n",
    "        car_bbox2d = (x-w//2,y-h//2,x+w//2,y+h//2)\n",
    "        car_bbox3d = None\n",
    "        all_detections = []\n",
    "        all_detections.append(obj_detection('car_1', car_loc3d, car_loc2d, car_bbox3d, car_bbox2d))\n",
    "        all_detection_info = construct_all_detection_info(\n",
    "            current_frame, cam_segment_mapping, ego_trajectory, ego_config, all_detections)\n",
    "    if all_detection_info:\n",
    "        print(all_detection_info[0].road_type)\n",
    "    next_sample_plan = generate_sample_plan(video, next_frame_num, all_detection_info,  50)\n",
    "    next_frame = None\n",
    "    if next_sample_plan.get_next_sample_frame_info():\n",
    "        next_sample_frame_name, next_sample_frame_num, next_sample_frame_time = (\n",
    "            next_sample_plan.get_next_sample_frame_info())\n",
    "        print(\"next frame name\", next_sample_frame_name)\n",
    "        print(\"next frame num\", next_sample_frame_num)\n",
    "    #     print(next_sample_plan.action)\n",
    "        next_frame = cv2.imread(test_img_base_dir+next_sample_frame_name)\n",
    "#         cv2.imshow(\"next_frame\", next_frame)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "    return next_sample_plan, next_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40ad4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_estimated_all_detection_info(current_frame, cam_segment_mapping, ego_config, ego_trajectory):\n",
    "    all_detections = []\n",
    "    full_img_detection = sample_frame_tracker.run(current_frame, save_vid=True, detect_only=True)\n",
    "#     display_detection(current_frame, full_img_detection)\n",
    "    for obj_idx, detection in full_img_detection.items():\n",
    "        obj_cls, bbox = detection\n",
    "        x,y,w,h = list(map(int,bbox))\n",
    "        car_loc2d = (x,y+h//2)\n",
    "#         print(car_loc2d)\n",
    "        car_bbox2d = (x-w//2,y-h//2,x+w//2,y+h//2)\n",
    "        car_bbox3d = None\n",
    "        estimate_3d = detection_to_img_segment(car_loc2d, cam_segment_mapping)\n",
    "        if estimate_3d and estimate_3d.road_segment_info.segment_type in ['lane', 'laneSection']:\n",
    "            car_loc3d = tuple(Polygon(estimate_3d.road_segment_info.segment_polygon).centroid.coords)\n",
    "#             print(tuple(car_loc3d))\n",
    "            all_detections.append(obj_detection('car_1', car_loc3d, car_loc2d, car_bbox3d, car_bbox2d))\n",
    "    print(\"all_detections\", all_detections)\n",
    "    all_detection_info = construct_all_detection_info(\n",
    "        current_frame, cam_segment_mapping, ego_trajectory, ego_config, all_detections)\n",
    "    return all_detection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f2624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dry_run(sorted_ego_configs, start_frame_num, ego_trajectory, video):\n",
    "    skipped_frame_num = []\n",
    "    current_frame_num = start_frame_num\n",
    "    action_type_counts = {}\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    display_video = cv2.VideoWriter(f'sampled_frames_{video.replace(\"/\", \"_\")}.avi',fourcc, 10, (1600, 900))\n",
    "    start_time = time.time()\n",
    "    total_detection_time = 0\n",
    "    total_sample_plan_time = 0\n",
    "    for i in range(len(sorted_ego_configs)-1):\n",
    "        current_ego_config = sorted_ego_configs[i]\n",
    "        next_frame_num = sorted_ego_configs[i+1]['frameNum']\n",
    "        if current_ego_config['frameNum'] != current_frame_num:\n",
    "            skipped_frame_num.append(current_ego_config['frameNum'])\n",
    "            continue\n",
    "        cam_segment_mapping = map_imgsegment_roadsegment(current_ego_config)\n",
    "        current_frame = test_img_base_dir + current_ego_config['fileName']\n",
    "        display_video.write(cv2.imread(current_frame))\n",
    "        start_detection_time = time.time()\n",
    "        all_detection_info = construct_estimated_all_detection_info(\n",
    "            current_frame, cam_segment_mapping, current_ego_config, ego_trajectory)\n",
    "        total_detection_time += time.time()-start_detection_time\n",
    "        start_generate_sample_plan = time.time()\n",
    "        next_sample_plan, next_frame = generate_sample_plan_once(\n",
    "            video, current_ego_config, cam_segment_mapping, next_frame_num, all_detection_info=all_detection_info)\n",
    "        total_sample_plan_time += time.time() - start_generate_sample_plan\n",
    "        next_action_type = next_sample_plan.get_action_type()\n",
    "        if next_action_type not in action_type_counts:\n",
    "            action_type_counts[next_action_type] = 1\n",
    "        else:\n",
    "            action_type_counts[next_action_type] += 1\n",
    "        current_frame_num = next_sample_plan.get_next_frame_num(next_frame_num)\n",
    "    display_video.release()\n",
    "    print(\"sorted_ego_config_length\", len(sorted_ego_configs))\n",
    "    print(\"number of skipped\", len(skipped_frame_num))\n",
    "    print(skipped_frame_num)\n",
    "    print(action_type_counts)\n",
    "    total_run_time = time.time()-start_time\n",
    "    num_runs = len(sorted_ego_configs) - len(skipped_frame_num)\n",
    "    print(\"total_run_time\", total_run_time)\n",
    "    print(\"avg run time\", total_run_time/num_runs)\n",
    "    print(\"total_detection_time\", total_detection_time)\n",
    "    print(\"avg detection time\", total_detection_time/num_runs)\n",
    "    print(\"total_generate_sample_plan_time\", total_sample_plan_time)\n",
    "    print(\"avg generate_sample_plan time\", total_sample_plan_time/num_runs)\n",
    "    total_mapping_time = total_run_time-total_detection_time-total_sample_plan_time\n",
    "    print(\"total_mapping_time\", total_mapping_time)\n",
    "    print(\"avg mapping time\", total_mapping_time/num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b25f486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.3097507953643799\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections [obj_detection(id='car_1', car_loc3d=((1810.3982084033205, 860.0461031162465),), car_loc2d=(1058, 532), car_bbox3d=None, car_bbox2d=(970, 466, 1146, 532))]\n",
      "really contained True\n",
      "area 367.7220652969542\n",
      "lane\n",
      "ego_exit_segment_action action type: ego_exit_segment,\n",
      "        start time: 2018-08-27 08:51:32.112404+00:00,\n",
      "        finish time: 2018-08-27 08:51:32.912404+00:00,\n",
      "        start loc: [1772.1438, 866.30286],\n",
      "        end loc: [1778.386, 866.4177, 0.0]\n",
      "        estimated time: 0:00:00.800000\n",
      "next frame name sweeps/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385092362404.jpg\n",
      "next frame num 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.25127482414245605\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.2295975685119629\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.32300782203674316\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.2568855285644531\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.19021248817443848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.18526434898376465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections [obj_detection(id='car_1', car_loc3d=((1884.6902160362843, 872.6426751594242),), car_loc2d=(785, 502), car_bbox3d=None, car_bbox2d=(769, 484, 801, 502))]\n",
      "really contained True\n",
      "area 367.7220652969542\n",
      "lane\n",
      "ego_exit_segment_action action type: ego_exit_segment,\n",
      "        start time: 2018-08-27 08:51:32.762404+00:00,\n",
      "        finish time: 2018-08-27 08:51:32.912404+00:00,\n",
      "        start loc: [1777.1527, 866.3999],\n",
      "        end loc: [1778.386, 866.4177, 0.0]\n",
      "        estimated time: 0:00:00.150000\n",
      "next frame name sweeps/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385092912404.jpg\n",
      "next frame num 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.1861588954925537\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.1829850673675537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.14785146713256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections [obj_detection(id='car_1', car_loc3d=((1810.3982084033205, 860.0461031162465),), car_loc2d=(1230, 554), car_bbox3d=None, car_bbox2d=(1161, 474, 1299, 554))]\n",
      "really contained True\n",
      "area 119.17133613894602\n",
      "lane\n",
      "ego_exit_segment_action action type: ego_exit_segment,\n",
      "        start time: 2018-08-27 08:51:33.112404+00:00,\n",
      "        finish time: 2018-08-27 08:51:34.112404+00:00,\n",
      "        start loc: [1780.0085, 866.44666],\n",
      "        end loc: [1788.544, 866.5991, 0.0]\n",
      "        estimated time: 0:00:01\n",
      "next frame name sweeps/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385093362404.jpg\n",
      "next frame num 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.18590164184570312\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.18841099739074707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections [obj_detection(id='car_1', car_loc3d=((1810.3982084033205, 860.0461031162465),), car_loc2d=(1319, 562), car_bbox3d=None, car_bbox2d=(1250, 476, 1388, 562))]\n",
      "really contained True\n",
      "area 119.17133613894602\n",
      "lane\n",
      "ego_exit_segment_action action type: ego_exit_segment,\n",
      "        start time: 2018-08-27 08:51:33.412409+00:00,\n",
      "        finish time: 2018-08-27 08:51:34.112404+00:00,\n",
      "        start loc: [1782.5449, 866.48914],\n",
      "        end loc: [1788.544, 866.5991, 0.0]\n",
      "        estimated time: 0:00:00.699995\n",
      "next frame name sweeps/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385093662404.jpg\n",
      "next frame num 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.1604468822479248\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.15124034881591797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.14730405807495117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.1714010238647461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.1436760425567627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections [obj_detection(id='car_1', car_loc3d=((1884.6902160362843, 872.6426751594242),), car_loc2d=(777, 496), car_bbox3d=None, car_bbox2d=(760, 474, 794, 496))]\n",
      "really contained True\n",
      "area 119.17133613894602\n",
      "lane\n",
      "ego_exit_segment_action action type: ego_exit_segment,\n",
      "        start time: 2018-08-27 08:51:34.012404+00:00,\n",
      "        finish time: 2018-08-27 08:51:34.112404+00:00,\n",
      "        start loc: [1787.6869, 866.5818],\n",
      "        end loc: [1788.544, 866.5991, 0.0]\n",
      "        estimated time: 0:00:00.100000\n",
      "next frame name samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385094112404.jpg\n",
      "next frame num 317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total mapping time:  0.17557048797607422\n",
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "all_detections []\n",
      "total mapping time:  0.16657543182373047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/detection_estimation/utils.py:45: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  a, b = line.boundary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_video1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCAM_FRONT/n008-2018-08-27\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m sorted_ego_configs1, ego_trajectory1 \u001b[38;5;241m=\u001b[39m prepare_ego(test_video1)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdry_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_ego_configs1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mego_trajectory1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_video1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [33], line 20\u001b[0m, in \u001b[0;36mdry_run\u001b[0;34m(sorted_ego_configs, start_frame_num, ego_trajectory, video)\u001b[0m\n\u001b[1;32m     18\u001b[0m display_video\u001b[38;5;241m.\u001b[39mwrite(cv2\u001b[38;5;241m.\u001b[39mimread(current_frame))\n\u001b[1;32m     19\u001b[0m start_detection_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m all_detection_info \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_estimated_all_detection_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam_segment_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_ego_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mego_trajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m total_detection_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_detection_time\n\u001b[1;32m     23\u001b[0m start_generate_sample_plan \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [32], line 3\u001b[0m, in \u001b[0;36mconstruct_estimated_all_detection_info\u001b[0;34m(current_frame, cam_segment_mapping, ego_config, ego_trajectory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_estimated_all_detection_info\u001b[39m(current_frame, cam_segment_mapping, ego_config, ego_trajectory):\n\u001b[1;32m      2\u001b[0m     all_detections \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     full_img_detection \u001b[38;5;241m=\u001b[39m \u001b[43msample_frame_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     display_detection(current_frame, full_img_detection)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj_idx, detection \u001b[38;5;129;01min\u001b[39;00m full_img_detection\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet/sample_frame_tracker.py:100\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(source, conf_thres, iou_thres, max_det, show_vid, save_conf, save_crop, save_vid, nosave, classes, agnostic_nms, augment, visualize, update, line_thickness, hide_labels, hide_conf, hide_class, detect_only)\u001b[0m\n\u001b[1;32m     97\u001b[0m strongsort_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_sources):\n\u001b[1;32m     99\u001b[0m     strongsort_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 100\u001b[0m         \u001b[43mStrongSORT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstrong_sort_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_DIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_iou_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_IOU_DISTANCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_AGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_INIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnn_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNN_BUDGET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmc_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMC_LAMBDA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mema_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRONGSORT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMA_ALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     strongsort_list[i]\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mwarmup()\n\u001b[1;32m    115\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m nr_sources\n",
      "File \u001b[0;32m~/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet/strong_sort/strong_sort.py:34\u001b[0m, in \u001b[0;36mStrongSORT.__init__\u001b[0;34m(self, model_weights, device, fp16, max_dist, max_iou_distance, max_age, n_init, nn_budget, mc_lambda, ema_alpha)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     23\u001b[0m              model_weights,\n\u001b[1;32m     24\u001b[0m              device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m              ema_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     32\u001b[0m             ):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mReIDDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_dist \u001b[38;5;241m=\u001b[39m max_dist\n\u001b[1;32m     37\u001b[0m     metric \u001b[38;5;241m=\u001b[39m NearestNeighborDistanceMetric(\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_dist, nn_budget)\n",
      "File \u001b[0;32m~/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet/strong_sort/reid_multibackend.py:50\u001b[0m, in \u001b[0;36mReIDDetectMultiBackend.__init__\u001b[0;34m(self, weights, device, fp16)\u001b[0m\n\u001b[1;32m     47\u001b[0m         show_downloadeable_models()\n\u001b[1;32m     48\u001b[0m         exit()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# get rid of dataset information DeepSort model name\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m fp16 \u001b[38;5;28;01melse\u001b[39;00m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet/strong_sort/deep/reid/torchreid/utils/feature_extractor.py:88\u001b[0m, in \u001b[0;36mFeatureExtractor.__init__\u001b[0;34m(self, model_name, model_path, image_size, pixel_mean, pixel_std, pixel_norm, device, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- flops: \u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(flops))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mand\u001b[39;00m check_isfile(model_path):\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Build transform functions\u001b[39;00m\n\u001b[1;32m     91\u001b[0m transforms \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet/strong_sort/deep/reid/torchreid/utils/torchtools.py:294\u001b[0m, in \u001b[0;36mload_pretrained_weights\u001b[0;34m(model, weight_path)\u001b[0m\n\u001b[1;32m    291\u001b[0m         discarded_layers\u001b[38;5;241m.\u001b[39mappend(k)\n\u001b[1;32m    293\u001b[0m model_dict\u001b[38;5;241m.\u001b[39mupdate(new_state_dict)\n\u001b[0;32m--> 294\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matched_layers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    297\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe pretrained weights \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cannot be loaded, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease check the key names manually \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(** ignored and continue **)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_path)\n\u001b[1;32m    301\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1590\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   1585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1588\u001b[0m         )\n\u001b[0;32m-> 1590\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1578\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1578\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1578\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1578\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 1578 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1578\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1578\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1574\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(module, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1573\u001b[0m     local_metadata \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(prefix[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], {})\n\u001b[0;32m-> 1574\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1577\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/apperception_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1528\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m extra_state_key:\n\u001b[1;32m   1529\u001b[0m             input_name \u001b[38;5;241m=\u001b[39m key[\u001b[38;5;28mlen\u001b[39m(prefix):]\n\u001b[1;32m   1530\u001b[0m             input_name \u001b[38;5;241m=\u001b[39m input_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# get the name of param/buffer/child\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_video1 = 'CAM_FRONT/n008-2018-08-27'\n",
    "sorted_ego_configs1, ego_trajectory1 = prepare_ego(test_video1)\n",
    "dry_run(sorted_ego_configs1, 2, ego_trajectory1, test_video1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133465f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_video2 = 'CAM_FRONT/n008-2018-08-01'\n",
    "sorted_ego_configs2, ego_trajectory2 = prepare_ego(test_video2)\n",
    "dry_run(sorted_ego_configs2, 2, ego_trajectory2, test_video2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ef874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef986073a7322f2daa7cef2e5604e6018e5522cc159657af8e7aa863491a7631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
