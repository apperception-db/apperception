{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6bd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ego = Car on drivableRoad,\n",
    "#         facing Range(-15, 15) deg relative to roadDirection,\n",
    "#         with visibleDistance 50,\n",
    "#         with viewAngle 135 deg\n",
    "\n",
    "# other1 = Car on intersection,\n",
    "#             facing Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# other2 = Car on intersection,\n",
    "#             facing -1 * Range(50, 135) deg relative to ego.heading\n",
    "\n",
    "# require abs(relative heading of other1 from other2) > 100 deg\n",
    "# require (distance from ego to intersectionRegion) < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e70533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat do we have?\\nNow everything still in the mobilitydb\\n\\n1. Test on mini, CAM_FRONT images treated as raw videos\\n2. camera config for all the CAM_FRONT ego, each row is a single camera also a single image\\n3. road network ingested in the way for scenic case, may need to dig out other useful stuff\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F\n",
    "\n",
    "### Constants ###\n",
    "SAMPLING_RATE = 2\n",
    "CAMERA_ID = \"scene-0757\"\n",
    "\n",
    "CAMERA_COLUMNS = [\n",
    "    \"cameraId\",\n",
    "    \"frameId\",\n",
    "    \"frameNum\",\n",
    "    \"filename\",\n",
    "    \"cameraTranslation\",\n",
    "    \"cameraRotation\",\n",
    "    \"cameraIntrinsic\",\n",
    "    \"egoTranslation\",\n",
    "    \"egoRotation\",\n",
    "    \"timestamp\",\n",
    "    \"cameraHeading\",\n",
    "    \"egoHeading\",\n",
    "    \"cameraTranslationAbs\"] #road_direction not included yet\n",
    "\n",
    "def convert_frame_to_map(frame):\n",
    "    map_frame = dict(zip(CAMERA_COLUMNS, frames[:12]))\n",
    "    return map_frame\n",
    "\n",
    "def transform_to_world(frame_coordinate, ego_translation, ego_rotation):\n",
    "    ### TODO: get world coordinates\n",
    "    return frame_coordinate\n",
    "\n",
    "def get_obj_trajectory(tracking_df, ego_config):\n",
    "    '''\n",
    "    returned object info is a dictionary that looks like this:\n",
    "    {object_id:{frame_idx:[], #need to use the frame idx of the video to get the camera config for each frame\n",
    "                trajectory:[]}\n",
    "    '''\n",
    "    obj_info = {}\n",
    "    grouped_trajectory = tracking_df.groupby(by=[\"object_id\"])\n",
    "    for name, group in grouped_trajectory:\n",
    "        obj_info[name] = {}\n",
    "        group.frame_idx = group.frame_idx.add(-1)\n",
    "        object_df = group[[\n",
    "            'frame_idx', 'object_id', 'object_type', 'bbox_left', 'bbox_top', 'bbox_w', 'bbox_h']]\n",
    "        object_df = object_df.reset_index(drop=True)\n",
    "        framenums = group.frame_idx.tolist()\n",
    "        \n",
    "        ### get ego_config for each framenum\n",
    "        transformation_config = ego_config.iloc[framenums]\n",
    "        transformation_config = transformation_config.reset_index(drop=True)\n",
    "        \n",
    "        object_with_ego = pd.concat([object_df, transformation_config], axis=1)\n",
    "        ### for each coordinate, transform\n",
    "        obj_trajectory = []\n",
    "        for index, row in object_with_ego.iterrows():\n",
    "            obj_trajectory.append(transform_to_world(frame_coordinate=((row['bbox_left'], row['bbox_top']), \n",
    "                                                 (row['bbox_left']+row['bbox_w'], \n",
    "                                                  row['bbox_top']+row['bbox_h'])), \n",
    "                               ego_translation=row['egoTranslation'],\n",
    "                               ego_rotation=row['egoRotation']))\n",
    "        \n",
    "        obj_info[name]['frame_idx'] = object_with_ego[['frame_idx']]\n",
    "        obj_info[name]['trajectory'] = obj_trajectory\n",
    "    return obj_info\n",
    "\n",
    "\n",
    "def facing_relative(prev_traj_point, next_traj_point, current_ego_heading):\n",
    "    ### TODO: get direction from adjacent traj points, then calculate the relative degree\n",
    "    ####### COMPLETE\n",
    "    diff = next_traj_point - prev_traj_point\n",
    "    diff_heading = math.degrees(np.arctan2(diff[1], diff[0])) - 90\n",
    "    result = ((diff_heading - current_ego_heading) % 360 + 360) % 360\n",
    "    return result\n",
    "\n",
    "def facing_relative_check(obj_info, threshold):\n",
    "    for obj_id in obj_info:\n",
    "        frame_idx = obj_info[obj_id]['frame_idx'].frame_idx\n",
    "        trajectory = obj_info[obj_id]['trajectory']\n",
    "        ego_heading = ego_config.iloc[frame_idx.tolist()].egoHeading.tolist()\n",
    "        print(frame_idx[:len(ego_heading)-1][[facing_relative(trajectory[i], trajectory[i+1], ego_heading[i]) > threshold for i in range(len(ego_heading)-1)]])\n",
    "\n",
    "\"\"\"\n",
    "Dummy Modules for playing around\n",
    "\"\"\"\n",
    "class optimizeRoadNetwork:\n",
    "    def __init__(self, road_network=None):\n",
    "        self.road_network = road_network\n",
    "        self.optimized_road_network = self.optimize_road_network()\n",
    "\n",
    "    def optimize_road_network(self):\n",
    "        return self.road_network\n",
    "\n",
    "    def optimize_filter_intersection(self, sample_frames):\n",
    "        intersection_filtered = []\n",
    "\n",
    "        # TODO: Connection to DB for each execution might take too much time, do all at same time\n",
    "        for frame in sampled_frames:\n",
    "            map_frame = convert_frame_to_map(frame)\n",
    "            # use sql in order to make use of mobilitydb features. TODO: Find python alternative\n",
    "            query = f\"SELECT TRUE WHERE minDistance('{map_frame['egoTranslation']}', 'intersection') < 10\" \n",
    "            result = database._execute_query(query)\n",
    "            if result:\n",
    "                intersection_filtered.append(frame)\n",
    "\n",
    "        return intersection_filtered\n",
    "\n",
    "\n",
    "class optimizeSampling:\n",
    "    def __init__(self, sampling=None):\n",
    "        self.sampling = sampling\n",
    "        self.optimized_sampling = self.optimize_sampling()\n",
    "\n",
    "    def optimize_sampling(self):\n",
    "        return self.sampling\n",
    "    \n",
    "    def native_sample(self):\n",
    "        # Sample All Frames from Video at a #\n",
    "        query = f\"SELECT * FROM Cameras WHERE cameraId = '{CAMERA_ID}' ORDER BY frameNum\"\n",
    "\n",
    "        all_frames = database._execute_query(query)\n",
    "        sampled_frames = all_frames[::SAMPLING_RATE]\n",
    "        return sampled_frames\n",
    "\n",
    "\n",
    "class optimizeDecoding:\n",
    "    def __init__(self, decoding=None):\n",
    "        self.decoding = decoding\n",
    "        self.optimized_decoding = self.optimize_decoding()\n",
    "\n",
    "    def optimize_decoding(self):\n",
    "        return self.decoding\n",
    "\n",
    "    def optimize_tracking(self, lst_of_frames):\n",
    "        # Now the result is written to a txt file, need to fix this later\n",
    "        from Yolov5_Strong_Detection import sample_frame_tracker\n",
    "        for frames in lst_of_frames:\n",
    "            result = sample_frame_tracker.run(frames, save_vid=True)\n",
    "\n",
    "\"\"\"\n",
    "End to End Optimization Ingestion\n",
    "Use Case: select cars appears in intersection\n",
    "                      facing degree d relative to ego\n",
    "Test Data: only use one video as the test data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class optimizeIngestion:\n",
    "    def __init__(self):\n",
    "        self.optimize_road_network = optimizeRoadNetwork()\n",
    "        self.optimize_sampling = optimizeSampling()\n",
    "        self.optimize_decoding = optimizeDecoding()\n",
    "\n",
    "    def run_test(self):\n",
    "        # 1. Get all frames from video\n",
    "        all_frames = self.optimize_sampling.native_sample()\n",
    "        \n",
    "        \n",
    "        # 2. Filter out frames that in intersection\n",
    "        intersection_filtered = self.optimize_road_network.optimize_filter_intersection(all_frames)\n",
    "        ###TODO:fetch the camera_config corresponding to intersection_filtered \n",
    "        query = \"\"\"SELECT * FROM Cameras \n",
    "                    WHERE filename like 'samples/CAM_FRONT/%2018-08-01-15%' \n",
    "                    ORDER BY frameNum\"\"\" \n",
    "        camera_config = database._execute_query(query)\n",
    "        camera_config_df = pd.DataFrame(camera_config, columns=camera_columns)\n",
    "        ego_config = camera_config_df[['egoTranslation', 'egoRotation', 'egoHeading']]\n",
    "        \n",
    "        \n",
    "        # 3. Decode filtered_frames and track\n",
    "        self.optimize_decoding.optimize_tracking([intersection_filtered])\n",
    "        df = pd.read_csv(\"../optimization_playground/tracks/CAM_FRONT.txt\", sep=\" \", header=None, \n",
    "                 names=[\"frame_idx\", \n",
    "                        \"object_id\", \n",
    "                        \"bbox_left\", \n",
    "                        \"bbox_top\", \n",
    "                        \"bbox_w\", \n",
    "                        \"bbox_h\", \n",
    "                        \"None1\",\n",
    "                        \"None2\",\n",
    "                        \"None3\",\n",
    "                        \"None\",\n",
    "                        \"object_type\"])\n",
    "        \n",
    "        obj_info = get_obj_trajectory(df)\n",
    "        facing_relative_check(obj_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f44df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception/Yolov5_StrongSORT_OSNet\n"
     ]
    }
   ],
   "source": [
    "%cd ../Yolov5_StrongSORT_OSNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fd3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = sorted(glob.glob('/home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64558171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frames = [f for f in files if '2018-08-01-15' in f]\n",
    "len(test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627076c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "strong_sort/deep/reid/torchreid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n",
      "/home/yongming/.pyenv/versions/3.8.3/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "YOLOv5 ðŸš€ 2022-8-20 Python-3.8.3 torch-1.12.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strong_sort/configs/strong_sort.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "import sample_frame_tracker\n",
    "def track_sample(lst_of_frames):\n",
    "    for frames in lst_of_frames:\n",
    "        result = sample_frame_tracker.run(frames, save_vid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb82553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151603512404.jpg: 384x640 9 persons, 6 cars, 4 traffic lights, Done. YOLO:(0.253s), StrongSORT:(0.569s)\n",
      "image 2/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151604012404.jpg: 384x640 9 persons, 7 cars, 3 traffic lights, 1 fire hydrant, Done. YOLO:(0.254s), StrongSORT:(0.549s)\n",
      "image 3/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151604512404.jpg: 384x640 10 persons, 8 cars, 5 traffic lights, Done. YOLO:(0.245s), StrongSORT:(0.796s)\n",
      "image 4/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151605012404.jpg: 384x640 8 persons, 11 cars, 6 traffic lights, 1 fire hydrant, Done. YOLO:(0.321s), StrongSORT:(0.717s)\n",
      "image 5/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151605512404.jpg: 384x640 7 persons, 13 cars, 2 traffic lights, 1 fire hydrant, 1 clock, Done. YOLO:(0.231s), StrongSORT:(0.619s)\n",
      "image 6/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151606012404.jpg: 384x640 3 persons, 13 cars, 3 traffic lights, Done. YOLO:(0.228s), StrongSORT:(0.547s)\n",
      "image 7/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151606512404.jpg: 384x640 3 persons, 11 cars, 2 traffic lights, Done. YOLO:(0.254s), StrongSORT:(0.460s)\n",
      "image 8/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151607012404.jpg: 384x640 2 persons, 12 cars, 2 traffic lights, Done. YOLO:(0.226s), StrongSORT:(0.476s)\n",
      "image 9/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151607512404.jpg: 384x640 2 persons, 12 cars, 1 traffic light, Done. YOLO:(0.214s), StrongSORT:(0.389s)\n",
      "image 10/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151608012404.jpg: 384x640 3 persons, 13 cars, 1 traffic light, Done. YOLO:(0.241s), StrongSORT:(0.430s)\n",
      "image 11/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151608512404.jpg: 384x640 1 person, 15 cars, 1 traffic light, Done. YOLO:(0.226s), StrongSORT:(0.405s)\n",
      "image 12/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609012404.jpg: 384x640 2 persons, 15 cars, 1 traffic light, Done. YOLO:(0.211s), StrongSORT:(0.460s)\n",
      "image 13/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609512404.jpg: 384x640 1 person, 14 cars, 1 traffic light, Done. YOLO:(0.235s), StrongSORT:(0.441s)\n",
      "image 14/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609912404.jpg: 384x640 2 persons, 13 cars, 1 traffic light, Done. YOLO:(0.241s), StrongSORT:(0.419s)\n",
      "image 15/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151610412404.jpg: 384x640 3 persons, 16 cars, Done. YOLO:(0.211s), StrongSORT:(0.496s)\n",
      "image 16/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151610912404.jpg: 384x640 3 persons, 15 cars, Done. YOLO:(0.217s), StrongSORT:(0.460s)\n",
      "image 17/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151611412404.jpg: 384x640 2 persons, 13 cars, Done. YOLO:(0.231s), StrongSORT:(0.393s)\n",
      "image 18/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151611862404.jpg: 384x640 2 persons, 12 cars, Done. YOLO:(0.222s), StrongSORT:(0.371s)\n",
      "image 19/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151612362404.jpg: 384x640 2 persons, 11 cars, 1 traffic light, Done. YOLO:(0.228s), StrongSORT:(0.389s)\n",
      "image 20/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151612862404.jpg: 384x640 1 person, 13 cars, Done. YOLO:(0.228s), StrongSORT:(0.399s)\n",
      "image 21/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151613362404.jpg: 384x640 1 person, 16 cars, Done. YOLO:(0.219s), StrongSORT:(0.489s)\n",
      "image 22/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151613912404.jpg: 384x640 1 person, 13 cars, 1 truck, 1 traffic light, Done. YOLO:(0.221s), StrongSORT:(0.438s)\n",
      "image 23/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151614412404.jpg: 384x640 1 person, 2 bicycles, 16 cars, Done. YOLO:(0.256s), StrongSORT:(0.692s)\n",
      "image 24/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151614912404.jpg: 384x640 1 bicycle, 16 cars, 1 truck, Done. YOLO:(0.241s), StrongSORT:(0.466s)\n",
      "image 25/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151615412404.jpg: 384x640 1 person, 2 bicycles, 19 cars, 1 truck, Done. YOLO:(0.249s), StrongSORT:(0.605s)\n",
      "image 26/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151615912404.jpg: 384x640 1 person, 3 bicycles, 19 cars, 1 truck, 1 traffic light, Done. YOLO:(0.214s), StrongSORT:(0.622s)\n",
      "image 27/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151616412404.jpg: 384x640 1 person, 3 bicycles, 19 cars, 1 truck, Done. YOLO:(0.215s), StrongSORT:(0.603s)\n",
      "image 28/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151616912404.jpg: 384x640 1 person, 4 bicycles, 19 cars, 1 truck, Done. YOLO:(0.208s), StrongSORT:(0.598s)\n",
      "image 29/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151617362404.jpg: 384x640 1 person, 5 bicycles, 20 cars, 1 truck, Done. YOLO:(0.214s), StrongSORT:(0.639s)\n",
      "image 30/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151617912404.jpg: 384x640 1 person, 1 bicycle, 18 cars, 1 truck, Done. YOLO:(0.212s), StrongSORT:(0.529s)\n",
      "image 31/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151618412404.jpg: 384x640 1 person, 3 bicycles, 15 cars, 1 truck, Done. YOLO:(0.212s), StrongSORT:(0.533s)\n",
      "image 32/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151618912404.jpg: 384x640 2 persons, 18 cars, 1 truck, Done. YOLO:(0.208s), StrongSORT:(0.571s)\n",
      "image 33/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151619412404.jpg: 384x640 1 person, 1 bicycle, 20 cars, 1 truck, Done. YOLO:(0.219s), StrongSORT:(0.629s)\n",
      "image 34/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151619912458.jpg: 384x640 2 persons, 1 bicycle, 21 cars, 1 truck, Done. YOLO:(0.222s), StrongSORT:(0.677s)\n",
      "image 35/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151620412404.jpg: 384x640 18 cars, 1 truck, Done. YOLO:(0.245s), StrongSORT:(0.558s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 36/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621012404.jpg: 384x640 18 cars, 1 truck, Done. YOLO:(0.229s), StrongSORT:(0.555s)\n",
      "image 37/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621412404.jpg: 384x640 20 cars, 1 truck, Done. YOLO:(0.239s), StrongSORT:(0.564s)\n",
      "image 38/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621912404.jpg: 384x640 19 cars, Done. YOLO:(0.288s), StrongSORT:(0.704s)\n",
      "image 39/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151622412404.jpg: 384x640 1 bicycle, 18 cars, 1 umbrella, Done. YOLO:(0.212s), StrongSORT:(0.529s)\n",
      "image 40/40 /home/yongming/workspace/research/apperception/v1.0-mini/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151622912404.jpg: 384x640 21 cars, Done. YOLO:(0.234s), StrongSORT:(0.582s)\n",
      "Speed: 1.2ms pre-process, 231.8ms inference, 1.0ms NMS, 534.2ms strong sort update per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/tracks/\u001b[0m\n",
      "1 tracks saved to /home/yongming/workspace/research/apperception_new_local/apperception/optimization_playground/tracks/\n"
     ]
    }
   ],
   "source": [
    "track_sample([test_frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6d825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../optimization_playground/tracks/CAM_FRONT.txt\", sep=\" \", header=None, \n",
    "                 names=[\"frame_idx\", \n",
    "                        \"object_id\", \n",
    "                        \"bbox_left\", \n",
    "                        \"bbox_top\", \n",
    "                        \"bbox_w\", \n",
    "                        \"bbox_h\", \n",
    "                        \"None1\",\n",
    "                        \"None2\",\n",
    "                        \"None3\",\n",
    "                        \"None\",\n",
    "                        \"object_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317bdba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>object_id</th>\n",
       "      <th>bbox_left</th>\n",
       "      <th>bbox_top</th>\n",
       "      <th>bbox_w</th>\n",
       "      <th>bbox_h</th>\n",
       "      <th>None1</th>\n",
       "      <th>None2</th>\n",
       "      <th>None3</th>\n",
       "      <th>None</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>996</td>\n",
       "      <td>485</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1400</td>\n",
       "      <td>476</td>\n",
       "      <td>32</td>\n",
       "      <td>103</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1083</td>\n",
       "      <td>470</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>976</td>\n",
       "      <td>463</td>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1360</td>\n",
       "      <td>461</td>\n",
       "      <td>34</td>\n",
       "      <td>99</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>40</td>\n",
       "      <td>210</td>\n",
       "      <td>853</td>\n",
       "      <td>481</td>\n",
       "      <td>53</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>40</td>\n",
       "      <td>212</td>\n",
       "      <td>1564</td>\n",
       "      <td>525</td>\n",
       "      <td>35</td>\n",
       "      <td>188</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>40</td>\n",
       "      <td>233</td>\n",
       "      <td>264</td>\n",
       "      <td>484</td>\n",
       "      <td>153</td>\n",
       "      <td>120</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>40</td>\n",
       "      <td>239</td>\n",
       "      <td>932</td>\n",
       "      <td>479</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>40</td>\n",
       "      <td>241</td>\n",
       "      <td>161</td>\n",
       "      <td>493</td>\n",
       "      <td>173</td>\n",
       "      <td>113</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame_idx  object_id  bbox_left  bbox_top  bbox_w  bbox_h  None1  None2  \\\n",
       "0            3          1        996       485      61      49     -1     -1   \n",
       "1            3          4       1400       476      32     103     -1     -1   \n",
       "2            3         10       1083       470      41      40     -1     -1   \n",
       "3            4          1        976       463      67      50     -1     -1   \n",
       "4            4          4       1360       461      34      99     -1     -1   \n",
       "..         ...        ...        ...       ...     ...     ...    ...    ...   \n",
       "518         40        210        853       481      53      43     -1     -1   \n",
       "519         40        212       1564       525      35     188     -1     -1   \n",
       "520         40        233        264       484     153     120     -1     -1   \n",
       "521         40        239        932       479      37      38     -1     -1   \n",
       "522         40        241        161       493     173     113     -1     -1   \n",
       "\n",
       "     None3  None object_type  \n",
       "0       -1     0         car  \n",
       "1       -1     0      person  \n",
       "2       -1     0         car  \n",
       "3       -1     0         car  \n",
       "4       -1     0      person  \n",
       "..     ...   ...         ...  \n",
       "518     -1     0         car  \n",
       "519     -1     0         car  \n",
       "520     -1     0         car  \n",
       "521     -1     0         car  \n",
       "522     -1     0         car  \n",
       "\n",
       "[523 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b41193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yongming/workspace/research/apperception_new_local/apperception\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8c570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apperception.database import database\n",
    "from apperception.world import empty_world\n",
    "from apperception.utils import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad6b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_columns = [\n",
    "    \"cameraId\",\n",
    "    \"frameId\",\n",
    "    \"frameNum\",\n",
    "    \"filename\",\n",
    "    \"cameraTranslation\",\n",
    "    \"cameraRotation\",\n",
    "    \"cameraIntrinsic\",\n",
    "    \"egoTranslation\",\n",
    "    \"egoRotation\",\n",
    "    \"timestamp\",\n",
    "    \"cameraHeading\",\n",
    "    \"egoHeading\",\n",
    "    \"cameraTranslationAbs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece7c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample All Frames from Video at a #\n",
    "query = f\"SELECT * FROM Cameras WHERE filename like 'samples/CAM_FRONT/%2018-08-01-15%' ORDER BY frameNum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea6a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_config = database._execute_query(query)\n",
    "camera_config_df = pd.DataFrame(camera_config, columns=camera_columns)\n",
    "ego_config = camera_config_df[['egoTranslation', 'egoRotation', 'egoHeading']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_world(frame_coordinate, ego_translation, ego_rotation):\n",
    "    ### TODO: get world coordinates\n",
    "    return frame_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67ee1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_trajectory(tracking_df):\n",
    "    '''\n",
    "    returned object info is a dictionary that looks like this:\n",
    "    {object_id:{frame_idx:[], #need to use the frame idx of the video to get the camera config for each frame\n",
    "                trajectory:[]}\n",
    "    '''\n",
    "    obj_info = {}\n",
    "    grouped_trajectory = tracking_df.groupby(by=[\"object_id\"])\n",
    "    for name, group in grouped_trajectory:\n",
    "        obj_info[name] = {}\n",
    "        group.frame_idx = group.frame_idx.add(-1)\n",
    "        object_df = group[[\n",
    "            'frame_idx', 'object_id', 'object_type', 'bbox_left', 'bbox_top', 'bbox_w', 'bbox_h']]\n",
    "        object_df = object_df.reset_index(drop=True)\n",
    "        framenums = group.frame_idx.tolist()\n",
    "        \n",
    "        ### get ego_config for each framenum\n",
    "        transformation_config = ego_config.iloc[framenums]\n",
    "        transformation_config = transformation_config.reset_index(drop=True)\n",
    "        \n",
    "        object_with_ego = pd.concat([object_df, transformation_config], axis=1)\n",
    "        ### for each coordinate, transform\n",
    "        obj_trajectory = []\n",
    "        for index, row in object_with_ego.iterrows():\n",
    "            obj_trajectory.append(transform_to_world(frame_coordinate=((row['bbox_left'], row['bbox_top']), \n",
    "                                                 (row['bbox_left']+row['bbox_w'], \n",
    "                                                  row['bbox_top']+row['bbox_h'])), \n",
    "                               ego_translation=row['egoTranslation'],\n",
    "                               ego_rotation=row['egoRotation']))\n",
    "        \n",
    "        obj_info[name]['frame_idx'] = object_with_ego[['frame_idx']]\n",
    "        obj_info[name]['trajectory'] = obj_trajectory\n",
    "    return obj_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9664416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_info = get_obj_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81f9ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facing_relative(prev_traj_point, next_traj_point, current_ego_heading):\n",
    "    ### TODO: get direction from adjacent traj points, then calculate the relative degree\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0bc4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facing_relative_check(obj_info, threshold):\n",
    "    for obj_id in obj_info:\n",
    "        frame_idx = obj_info[obj_id]['frame_idx'].frame_idx\n",
    "        trajectory = obj_info[obj_id]['trajectory']\n",
    "        ego_heading = ego_config.iloc[frame_idx.tolist()].egoHeading.tolist()\n",
    "        print(frame_idx[:len(ego_heading)-1][[facing_relative(trajectory[i], trajectory[i+1], ego_heading[i]) > threshold for i in range(len(ego_heading)-1)]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62dd6024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      3\n",
      "2      4\n",
      "3      5\n",
      "4      6\n",
      "5      7\n",
      "6      8\n",
      "7      9\n",
      "8     10\n",
      "9     11\n",
      "10    12\n",
      "11    13\n",
      "12    14\n",
      "13    15\n",
      "14    16\n",
      "15    17\n",
      "16    18\n",
      "17    19\n",
      "18    20\n",
      "19    23\n",
      "20    24\n",
      "21    25\n",
      "22    26\n",
      "23    27\n",
      "24    28\n",
      "25    29\n",
      "26    30\n",
      "27    31\n",
      "28    32\n",
      "29    33\n",
      "30    34\n",
      "Name: frame_idx, dtype: int64\n",
      "0      2\n",
      "1      3\n",
      "2      4\n",
      "3      5\n",
      "4      6\n",
      "5      8\n",
      "6      9\n",
      "7     10\n",
      "8     11\n",
      "9     12\n",
      "10    13\n",
      "11    14\n",
      "12    15\n",
      "13    16\n",
      "14    18\n",
      "15    19\n",
      "16    20\n",
      "17    21\n",
      "18    22\n",
      "Name: frame_idx, dtype: int64\n",
      "0      2\n",
      "1      3\n",
      "2      4\n",
      "3      5\n",
      "4      6\n",
      "5      7\n",
      "6      8\n",
      "7      9\n",
      "8     10\n",
      "9     11\n",
      "10    12\n",
      "11    13\n",
      "12    14\n",
      "13    15\n",
      "14    16\n",
      "15    17\n",
      "16    18\n",
      "17    19\n",
      "18    20\n",
      "19    21\n",
      "20    29\n",
      "21    30\n",
      "22    34\n",
      "23    35\n",
      "24    36\n",
      "25    37\n",
      "26    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     3\n",
      "1     4\n",
      "2     5\n",
      "3     7\n",
      "4     8\n",
      "5     9\n",
      "6    10\n",
      "7    11\n",
      "8    12\n",
      "Name: frame_idx, dtype: int64\n",
      "0      6\n",
      "1      7\n",
      "2      8\n",
      "3      9\n",
      "4     10\n",
      "5     11\n",
      "6     12\n",
      "7     14\n",
      "8     15\n",
      "9     16\n",
      "10    17\n",
      "11    18\n",
      "12    25\n",
      "13    26\n",
      "14    27\n",
      "15    28\n",
      "16    30\n",
      "17    31\n",
      "18    32\n",
      "19    33\n",
      "20    34\n",
      "21    35\n",
      "22    36\n",
      "23    37\n",
      "24    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0      6\n",
      "1      7\n",
      "2     10\n",
      "3     11\n",
      "4     13\n",
      "5     14\n",
      "6     17\n",
      "7     18\n",
      "8     19\n",
      "9     21\n",
      "10    22\n",
      "11    23\n",
      "12    24\n",
      "13    25\n",
      "14    26\n",
      "15    27\n",
      "16    28\n",
      "17    29\n",
      "18    30\n",
      "19    35\n",
      "20    36\n",
      "21    37\n",
      "22    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     7\n",
      "1     8\n",
      "2     9\n",
      "3    10\n",
      "Name: frame_idx, dtype: int64\n",
      "0     10\n",
      "1     11\n",
      "2     12\n",
      "3     13\n",
      "4     14\n",
      "5     15\n",
      "6     16\n",
      "7     18\n",
      "8     19\n",
      "9     20\n",
      "10    21\n",
      "11    22\n",
      "12    23\n",
      "13    24\n",
      "Name: frame_idx, dtype: int64\n",
      "0     10\n",
      "1     11\n",
      "2     12\n",
      "3     13\n",
      "4     14\n",
      "5     15\n",
      "6     16\n",
      "7     21\n",
      "8     22\n",
      "9     23\n",
      "10    24\n",
      "11    25\n",
      "12    26\n",
      "13    27\n",
      "14    28\n",
      "15    29\n",
      "16    30\n",
      "17    31\n",
      "18    32\n",
      "19    33\n",
      "20    34\n",
      "21    35\n",
      "22    36\n",
      "23    37\n",
      "24    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    10\n",
      "1    11\n",
      "2    12\n",
      "3    13\n",
      "4    14\n",
      "5    15\n",
      "6    22\n",
      "Name: frame_idx, dtype: int64\n",
      "0     10\n",
      "1     11\n",
      "2     12\n",
      "3     13\n",
      "4     14\n",
      "5     15\n",
      "6     16\n",
      "7     17\n",
      "8     18\n",
      "9     19\n",
      "10    20\n",
      "11    21\n",
      "12    22\n",
      "13    23\n",
      "14    24\n",
      "15    25\n",
      "16    26\n",
      "17    27\n",
      "18    28\n",
      "19    29\n",
      "20    30\n",
      "21    31\n",
      "22    32\n",
      "23    33\n",
      "24    34\n",
      "25    35\n",
      "26    36\n",
      "27    37\n",
      "28    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     12\n",
      "1     13\n",
      "2     14\n",
      "3     15\n",
      "4     16\n",
      "5     17\n",
      "6     18\n",
      "7     25\n",
      "8     26\n",
      "9     27\n",
      "10    28\n",
      "11    29\n",
      "12    30\n",
      "13    31\n",
      "14    32\n",
      "15    33\n",
      "16    34\n",
      "17    35\n",
      "18    36\n",
      "19    37\n",
      "20    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    13\n",
      "1    14\n",
      "2    15\n",
      "3    17\n",
      "4    18\n",
      "5    19\n",
      "6    20\n",
      "7    21\n",
      "Name: frame_idx, dtype: int64\n",
      "0     13\n",
      "1     14\n",
      "2     15\n",
      "3     16\n",
      "4     18\n",
      "5     19\n",
      "6     20\n",
      "7     21\n",
      "8     22\n",
      "9     23\n",
      "10    24\n",
      "11    25\n",
      "12    26\n",
      "13    27\n",
      "14    28\n",
      "15    30\n",
      "16    31\n",
      "17    32\n",
      "18    33\n",
      "19    34\n",
      "20    35\n",
      "21    36\n",
      "22    37\n",
      "23    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     13\n",
      "1     14\n",
      "2     15\n",
      "3     16\n",
      "4     17\n",
      "5     18\n",
      "6     19\n",
      "7     20\n",
      "8     21\n",
      "9     22\n",
      "10    23\n",
      "11    24\n",
      "12    25\n",
      "13    26\n",
      "14    27\n",
      "15    28\n",
      "16    29\n",
      "17    30\n",
      "18    31\n",
      "19    32\n",
      "20    33\n",
      "21    34\n",
      "22    37\n",
      "Name: frame_idx, dtype: int64\n",
      "0     14\n",
      "1     15\n",
      "2     16\n",
      "3     17\n",
      "4     18\n",
      "5     19\n",
      "6     20\n",
      "7     21\n",
      "8     22\n",
      "9     23\n",
      "10    24\n",
      "11    25\n",
      "12    26\n",
      "13    27\n",
      "14    28\n",
      "15    29\n",
      "16    30\n",
      "17    31\n",
      "18    32\n",
      "19    33\n",
      "20    34\n",
      "21    35\n",
      "22    36\n",
      "23    37\n",
      "24    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     14\n",
      "1     15\n",
      "2     16\n",
      "3     19\n",
      "4     20\n",
      "5     21\n",
      "6     22\n",
      "7     23\n",
      "8     24\n",
      "9     25\n",
      "10    26\n",
      "11    27\n",
      "12    28\n",
      "13    29\n",
      "14    30\n",
      "15    31\n",
      "16    32\n",
      "17    33\n",
      "18    34\n",
      "19    35\n",
      "Name: frame_idx, dtype: int64\n",
      "0     16\n",
      "1     17\n",
      "2     18\n",
      "3     19\n",
      "4     20\n",
      "5     24\n",
      "6     25\n",
      "7     26\n",
      "8     27\n",
      "9     28\n",
      "10    29\n",
      "11    30\n",
      "12    31\n",
      "13    32\n",
      "14    33\n",
      "15    34\n",
      "16    35\n",
      "17    36\n",
      "18    37\n",
      "19    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     17\n",
      "1     18\n",
      "2     20\n",
      "3     21\n",
      "4     22\n",
      "5     23\n",
      "6     24\n",
      "7     25\n",
      "8     26\n",
      "9     27\n",
      "10    28\n",
      "11    29\n",
      "12    30\n",
      "13    31\n",
      "14    32\n",
      "15    33\n",
      "16    34\n",
      "17    35\n",
      "18    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     17\n",
      "1     20\n",
      "2     21\n",
      "3     22\n",
      "4     23\n",
      "5     24\n",
      "6     25\n",
      "7     26\n",
      "8     27\n",
      "9     28\n",
      "10    29\n",
      "11    30\n",
      "12    31\n",
      "13    32\n",
      "14    33\n",
      "15    34\n",
      "16    35\n",
      "17    36\n",
      "18    37\n",
      "Name: frame_idx, dtype: int64\n",
      "0    19\n",
      "1    20\n",
      "2    21\n",
      "3    25\n",
      "Name: frame_idx, dtype: int64\n",
      "0     21\n",
      "1     22\n",
      "2     23\n",
      "3     24\n",
      "4     25\n",
      "5     26\n",
      "6     27\n",
      "7     28\n",
      "8     29\n",
      "9     30\n",
      "10    31\n",
      "11    32\n",
      "12    35\n",
      "13    36\n",
      "14    37\n",
      "15    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     24\n",
      "1     25\n",
      "2     26\n",
      "3     27\n",
      "4     28\n",
      "5     29\n",
      "6     30\n",
      "7     31\n",
      "8     32\n",
      "9     33\n",
      "10    34\n",
      "11    35\n",
      "12    36\n",
      "13    37\n",
      "14    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    24\n",
      "1    25\n",
      "2    26\n",
      "3    27\n",
      "4    28\n",
      "5    30\n",
      "6    31\n",
      "7    32\n",
      "8    33\n",
      "Name: frame_idx, dtype: int64\n",
      "0    24\n",
      "1    25\n",
      "2    26\n",
      "3    27\n",
      "4    28\n",
      "5    29\n",
      "6    30\n",
      "7    33\n",
      "Name: frame_idx, dtype: int64\n",
      "0    25\n",
      "1    26\n",
      "2    27\n",
      "Name: frame_idx, dtype: int64\n",
      "0     28\n",
      "1     29\n",
      "2     30\n",
      "3     31\n",
      "4     32\n",
      "5     33\n",
      "6     34\n",
      "7     35\n",
      "8     36\n",
      "9     37\n",
      "10    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0     28\n",
      "1     29\n",
      "2     30\n",
      "3     31\n",
      "4     32\n",
      "5     33\n",
      "6     34\n",
      "7     35\n",
      "8     36\n",
      "9     37\n",
      "10    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    29\n",
      "1    31\n",
      "2    32\n",
      "3    33\n",
      "4    34\n",
      "5    35\n",
      "6    36\n",
      "7    37\n",
      "8    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    33\n",
      "1    35\n",
      "2    36\n",
      "3    37\n",
      "4    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    33\n",
      "1    36\n",
      "Name: frame_idx, dtype: int64\n",
      "0    36\n",
      "1    37\n",
      "2    38\n",
      "Name: frame_idx, dtype: int64\n",
      "0    36\n",
      "1    38\n",
      "Name: frame_idx, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "facing_relative_check(obj_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1778c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
